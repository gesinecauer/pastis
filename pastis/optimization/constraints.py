import sys

if sys.version_info[0] < 3:
    raise Exception("Must be using Python 3")

import numpy as np
import re

from .utils_poisson import _setup_jax
_setup_jax()
import jax.numpy as ag_np
from jax.nn import relu
from jax.scipy.stats.gamma import logpdf as logpdf_gamma
from jax.scipy.stats.norm import logpdf as logpdf_norm
# from jax.scipy.special import gammaln

from .multiscale_optimization import decrease_lengths_res, decrease_counts_res
from .multiscale_optimization import _count_fullres_per_lowres_bead
from .multiscale_optimization import _group_counts_multiscale
from .utils_poisson import find_beads_to_remove, _euclidean_distance
from .utils_poisson import relu_min, relu_max
from .utils_poisson import _inter_counts, _counts_near_diag, _intra_mask
from .counts import ambiguate_counts, _ambiguate_beta
from .likelihoods import poisson_nll, gamma_poisson_nll
from .poisson import get_gamma_params, get_gamma_moments
from ..io.read import load_data


class Constraint(object):
    """Compute loss for the given constraint.

    Prepares cand computes the loss function for the given constraint.

    Parameters
    ----------
    lambda_val : float
        Lambda that specifies how strongly constraint is applied during
        calculation of the entire objective.
    lengths : array_like of int
        Number of beads per homolog of each chromosome.
    ploidy : {1, 2}
        Ploidy, 1 indicates haploid, 2 indicates diploid.
    multiscale_factor : int, optional
        Factor by which to reduce the resolution. A value of 2 halves the
        resolution. A value of 1 indicates full resolution.
    hparams : dict, optional
        Any hyperparameters used for the calculation of constraint.

    Attributes
    ----------
    abbrev : str
        Three-letter abbreviation for constraint name.
    name : str
        Full name of constraint
    during_alpha_infer : bool
        Whether or not this constraint should be computed during inference of
        alpha.
    lambda_val : float
        Lambda that specifies how strongly constraint is applied during
        calculation of the entire objective.
    lengths : array_like of int
        Number of beads per homolog of each chromosome.
    ploidy : {1, 2}
        Ploidy, 1 indicates haploid, 2 indicates diploid.
    multiscale_factor : int, optional
        Factor by which to reduce the resolution. A value of 2 halves the
        resolution. A value of 1 indicates full resolution.
    hparams : dict, optional
        Any hyperparameters used for the calculation of constraint.
    fullres_struct_nan : array of int
        Beads that should be removed (set to NaN) in the full-res structure.
    lowmem : bool, optional
        Whether variables generated by _setup should be saved for subsequent
        iterations.
    """

    def __init__(self, lambda_val, lengths, ploidy, multiscale_factor=1,
                 hparams=None, fullres_struct_nan=None, lowmem=False, mods=[]):
        self.abbrev = None
        self.name = None
        self.during_alpha_infer = None
        self.lambda_val = lambda_val
        self.lengths = lengths
        self.ploidy = ploidy
        self.multiscale_factor = multiscale_factor
        self.hparams = hparams
        self._fullres_struct_nan = fullres_struct_nan  # For self._setup() only
        self._lowmem = lowmem
        self._var = None

    def __str__(self):
        out = [f"CONSTRAINT: {self.name},  LAMBDA={self.lambda_val:g}"]
        if self.hparams is None:
            return "\n".join(out)
        for name, val in self.hparams.items():
            label = f"\t\t\t{name} = "
            if isinstance(val, (np.ndarray, list)):
                out.append(label + np.array2string(
                    val, formatter={'float_kind': lambda x: "%.3g" % x},
                    prefix=" " * len(label), separator=", "))
            elif isinstance(val, float):
                out.append(f"{label}{val:g}")
            else:
                out.append(f"{label}{val}")
        return "\n".join(out)

    def check(self):
        """Check constraints object.

        Check that lambdas are greater than zero, and that necessary
        hyperparameters are supplied."""
        pass

    def _setup(self, counts=None, bias=None):
        """Set up for applying constraint (not specific to inferred params)"""
        pass

    def apply(self, struct, alpha=None, epsilon=None, counts=None, bias=None,
              inferring_alpha=False):

        """Apply constraint using given structure(s).

        Compute loss for the given constraint.

        Parameters
        ----------
        struct : ndarray
            3D chromatin structure(s) for which to compute the constraint.
        alpha : float
            Biophysical parameter of the transfer function used in converting
            counts to wish distances. If alpha is not specified, it will be
            inferred.
        epsilon : float, optional
            TODO
        counts : list of CountsMatrix subclass instances
            Preprocessed counts data.
        bias : array of float, optional
            Biases computed by ICE normalization.
        inferring_alpha : bool, optional
            A value of "True" indicates that the current optimization aims to
            infer alpha, rather than the structure.

        Returns
        -------
        constraint_obj
            Loss for constraint.
        """
        pass


class BeadChainConnectivity2019(Constraint):
    """TODO"""

    def __init__(self, lambda_val, lengths, ploidy, multiscale_factor=1,
                 hparams=None, fullres_struct_nan=None, lowmem=False, mods=[]):
        self.abbrev = "bcc"
        self.name = "Bead-chain connectivity (2019)"
        self.during_alpha_infer = False
        self.lambda_val = lambda_val
        self.lengths = np.asarray(lengths)
        self.lengths_lowres = decrease_lengths_res(
            self.lengths, multiscale_factor=multiscale_factor)
        self.ploidy = ploidy
        self.multiscale_factor = multiscale_factor
        self.hparams = hparams
        self._fullres_struct_nan = None  # Not necessary for this constraint
        self._lowmem = lowmem
        self._var = None
        self.mods = mods  # TODO remove

        self.check()

    def check(self):
        if self.lambda_val < 0:
            raise ValueError("Constraint lambda may not be < 0.")
        if self.hparams is not None and len(self.hparams) > 0:
            raise ValueError(f"{self.name} constraint may not have hyperparams")

    def _setup(self, counts=None, bias=None):
        if self.lambda_val <= 0:
            return
        if self._var is not None:
            return self._var
        row_nghbr = _neighboring_bead_indices(
            lengths=self.lengths, ploidy=self.ploidy,
            multiscale_factor=self.multiscale_factor)
        var = {'row_nghbr': row_nghbr}
        if not self._lowmem:
            self._var = var
        return var

    def apply(self, struct, alpha=None, epsilon=None, counts=None, bias=None,
              inferring_alpha=False):
        if self.lambda_val == 0 or (
                inferring_alpha and not self.during_alpha_infer):
            return 0.
        var = self._setup(counts=counts, bias=bias)

        nghbr_dis = _euclidean_distance(
            struct, row=var['row_nghbr'], col=var['row_nghbr'] + 1)
        n_edges = nghbr_dis.shape[0]
        nghbr_dis_var = n_edges * ag_np.square(
            nghbr_dis).sum() / ag_np.square(nghbr_dis.sum())
        obj = nghbr_dis_var - 1.

        if not ag_np.isfinite(obj):

            # if type(obj).__name__ in ('DeviceArray', 'ndarray'):
            print(f"{struct.mean()=:.3g}")  # TODO remove
            print(f"{np.isnan(struct).sum() / struct.size:.3g}")
            print(f"{nghbr_dis.mean()=:.3g}")
            print(f"{nghbr_dis.min()=:.3g}")
            print(f"{nghbr_dis_var.mean()=:.3g}")
            exit(1)

            raise ValueError(f"{self.name} constraint objective is {obj}.")
        return self.lambda_val * obj


class HomologSeparating2019(Constraint):
    """TODO"""

    def __init__(self, lambda_val, lengths, ploidy, multiscale_factor=1,
                 hparams=None, fullres_struct_nan=None, lowmem=False, mods=[]):
        self.abbrev = "hsc"
        self.name = "Homolog separating (2019)"
        self.during_alpha_infer = False
        self.lambda_val = lambda_val
        self.lengths = np.asarray(lengths)
        self.lengths_lowres = decrease_lengths_res(
            self.lengths, multiscale_factor=multiscale_factor)
        self.ploidy = ploidy
        self.multiscale_factor = multiscale_factor
        self.hparams = hparams
        self._fullres_struct_nan = fullres_struct_nan  # For self._setup() only
        self._lowmem = lowmem
        self._var = None
        self.mods = mods  # TODO remove

        self.check()

    def check(self):
        if self.ploidy == 1 and self.lambda_val > 0:
            raise ValueError(f"{self.name} constraint can not be applied to"
                             " haploid genomes.")
        if self.lambda_val < 0:
            raise ValueError("Constraint lambda may not be < 0.")
        # Hyperparam: perc_diff
        if self.hparams is None:
            self.hparams = {'perc_diff': None}
        if 'perc_diff' not in self.hparams:
            self.hparams['perc_diff'] = None
        if self.hparams['perc_diff'] is not None and (
                self.hparams['perc_diff'] < 0 or self.hparams['perc_diff'] > 1):
            raise ValueError("'perc_diff' must be between 0 and 1.")
        # Hyperparam: est_hmlg_sep
        if 'est_hmlg_sep' not in self.hparams or self.hparams[
                'est_hmlg_sep'] is None:
            raise ValueError(f"{self.name} constraint is missing neccessary"
                             " hyperparams: 'est_hmlg_sep'")
        if isinstance(self.hparams['est_hmlg_sep'], list):
            self.hparams['est_hmlg_sep'] = np.array(
                self.hparams['est_hmlg_sep'])
        if not isinstance(
                self.hparams['est_hmlg_sep'], (np.ndarray, float, int)):
            raise ValueError(f"{self.name} constraint hyperparam 'est_hmlg_sep'"
                             " not understood.")
        if isinstance(self.hparams['est_hmlg_sep'], np.ndarray):
            if self.hparams['est_hmlg_sep'].size not in (1, self.lengths.size):
                raise ValueError(f"{self.name} constraint hyperparam"
                                 " 'est_hmlg_sep' is not the correct size.")

    def _setup(self, counts=None, bias=None):
        if self.lambda_val <= 0:
            return
        if self._var is not None:
            return self._var

        if self.multiscale_factor > 1:
            fullres_per_lowres_bead = _count_fullres_per_lowres_bead(
                multiscale_factor=self.multiscale_factor, lengths=self.lengths,
                ploidy=self.ploidy, fullres_struct_nan=self._fullres_struct_nan)
            bead_weights = fullres_per_lowres_bead / self.multiscale_factor  # FIXME is this correct????
        else:
            bead_weights = np.ones((self.lengths_lowres.sum() * self.ploidy,))
        struct_nan = find_beads_to_remove(
            counts, lengths=self.lengths, ploidy=self.ploidy,
            multiscale_factor=self.multiscale_factor)
        if struct_nan.sum() != 0:
            raise ValueError("Check that we actually want to remove torm beads here...")
        bead_weights[struct_nan] = 0.
        n = self.lengths_lowres.sum()
        begin = end = 0
        for i in range(len(self.lengths_lowres)):
            end = end + self.lengths_lowres[i]
            bead_weights[:n][begin:end] /= np.sum(
                bead_weights[:n][begin:end])
            bead_weights[n:][begin:end] /= np.sum(
                bead_weights[n:][begin:end])
            begin = end
        bead_weights = bead_weights.reshape(-1, 1)

        var = {'bead_weights': bead_weights}
        if not self._lowmem:
            self._var = var
            self._fullres_struct_nan = None  # No longer needed unless lowmem
        return var

    def apply(self, struct, alpha=None, epsilon=None, counts=None, bias=None,
              inferring_alpha=False):
        if self.lambda_val == 0 or (
                inferring_alpha and not self.during_alpha_infer):
            return 0.
        var = self._setup(counts=counts, bias=bias)

        # Get homolog separation
        struct_bw = struct * np.repeat(var['bead_weights'], 3, axis=1)
        n = self.lengths_lowres.sum()
        hmlg_sep = ag_np.zeros(self.lengths_lowres.shape)
        begin = end = 0
        for i in range(self.lengths_lowres.size):
            end = end + ag_np.int32(self.lengths_lowres[i])
            chrom1_mean = ag_np.sum(struct_bw[begin:end], axis=0)
            chrom2_mean = ag_np.sum(struct_bw[(n + begin):(n + end)], axis=0)
            hmlg_sep_i = ag_np.sqrt(ag_np.sum(ag_np.square(
                chrom1_mean - chrom2_mean)))
            hmlg_sep = hmlg_sep.at[i].set(hmlg_sep_i)
            begin = end

        hmlg_sep_diff = self.hparams["est_hmlg_sep"] - hmlg_sep
        if 'rscale' in self.mods:
            hmlg_sep_diff = 1 - hmlg_sep / self.hparams["est_hmlg_sep"]
        if self.hparams['perc_diff'] is None:
            hmlg_sep_diff = relu(hmlg_sep_diff)
            raise ValueError("I thought we weren't doing RELU for HSC anymore")
        else:
            hsc_cutoff = ag_np.array(self.hparams['perc_diff'] * self.hparams[
                "est_hmlg_sep"])
            gt0 = hmlg_sep_diff > 0
            hmlg_sep_diff = hmlg_sep_diff.at[gt0].set(relu(
                hmlg_sep_diff[gt0] - hsc_cutoff[gt0]))
            hmlg_sep_diff = hmlg_sep_diff.at[~gt0].set(-relu(
                -(hmlg_sep_diff[~gt0] + hsc_cutoff[~gt0])))
        hmlg_sep_diff_sq = ag_np.square(hmlg_sep_diff)
        obj = ag_np.mean(hmlg_sep_diff_sq)

        if not ag_np.isfinite(obj):
            raise ValueError(f"{self.name} constraint objective is {obj}.")
        return self.lambda_val * obj


class BeadChainConnectivity2021(Constraint):
    """TODO"""

    def __init__(self, lambda_val, lengths, ploidy, multiscale_factor=1,
                 hparams=None, fullres_struct_nan=None, lowmem=False, mods=[]):
        self.abbrev = "bcc"
        self.name = "Bead-chain connectivity (2021)"
        self.during_alpha_infer = True
        self.lambda_val = lambda_val
        self.lengths = np.asarray(lengths)
        self.lengths_lowres = decrease_lengths_res(
            self.lengths, multiscale_factor=multiscale_factor)
        self.ploidy = ploidy
        self.multiscale_factor = multiscale_factor
        self.hparams = hparams
        self._fullres_struct_nan = fullres_struct_nan  # For self._setup() only
        self._lowmem = lowmem
        self._var = None
        self.mods = mods  # TODO remove

        self.check()

    def check(self):
        if self.lambda_val < 0:
            raise ValueError("Constraint lambda may not be < 0.")
        if self.hparams is not None and len(self.hparams) > 0:
            raise ValueError(f"{self.name} constraint may not have hyperparams")

    def _setup(self, counts=None, bias=None):
        if self.lambda_val <= 0:
            return
        if self._var is not None:
            return self._var
        row_nghbr = _neighboring_bead_indices(
            lengths=self.lengths, ploidy=self.ploidy,
            multiscale_factor=self.multiscale_factor)

        beta = _ambiguate_beta(
            [c.beta for c in counts if c.sum() != 0], counts=counts,
            lengths=self.lengths, ploidy=self.ploidy)
        counts_ambig = ambiguate_counts(
            counts=counts, lengths=self.lengths, ploidy=self.ploidy,
            exclude_zeros=True)
        row_nghbr_ambig = _neighboring_bead_indices(
            lengths=self.lengths, ploidy=1,
            multiscale_factor=self.multiscale_factor, counts=counts_ambig,
            include_struct_nan_beads=False)

        if bias is not None and not np.all(bias == 1):
            raise NotImplementedError("Implement for bias... especially for multires")  # TODO

        counts_ambig = decrease_counts_res(
            counts_ambig, multiscale_factor=self.multiscale_factor,
            lengths=self.lengths, ploidy=self.ploidy)
        nghbr_counts = counts_ambig.diagonal(k=1)[row_nghbr_ambig] / self.ploidy
        if self.multiscale_factor > 1:
            fullres_per_lowres_bead = _count_fullres_per_lowres_bead(
                multiscale_factor=self.multiscale_factor, lengths=self.lengths,
                ploidy=1, fullres_struct_nan=self._fullres_struct_nan)
            raise NotImplementedError("Implement for multiscale")  # TODO

        var = {
            'row_nghbr': row_nghbr, 'beta': beta, 'nghbr_counts': nghbr_counts}
        if not self._lowmem:
            self._var = var
            self._fullres_struct_nan = None  # No longer needed unless lowmem
        return var

    def apply(self, struct, alpha=None, epsilon=None, counts=None, bias=None,
              inferring_alpha=False):
        if self.lambda_val == 0 or (
                inferring_alpha and not self.during_alpha_infer):
            return 0.
        elif alpha is None:
            raise ValueError(f"Must input alpha for {self.name} constraint.")
        var = self._setup(counts=counts, bias=bias)

        if (var['nghbr_counts'] == 0).sum() == 0:
            nghbr_dis = np.power(var['nghbr_counts'] / var['beta'], 1 / alpha)
            mu = nghbr_dis.mean()
            sigma_max = nghbr_dis.std()
        else:
            mu, sigma_max = taylor_approx_ndc(
                var['nghbr_counts'], beta=var['beta'], alpha=alpha, order=1)

        nghbr_dis = _euclidean_distance(
            struct, row=var['row_nghbr'], col=var['row_nghbr'] + 1)

        mad = ag_np.median(ag_np.absolute(
            nghbr_dis - ag_np.median(nghbr_dis)))
        sigma_tmp = 1.4826 * mad
        # sigma_tmp = ag_np.sqrt(ag_np.mean(ag_np.square(data - mu))) # old - i guess this didn't work

        if 'norm_dis_nomin' in self.mods:
            sigma = sigma_tmp
        else:
            sigma = relu_min(sigma_tmp, sigma_max)

        obj = ag_np.log(sigma) + ag_np.mean(
            ag_np.square(nghbr_dis - mu)) / (2 * ag_np.square(sigma))
        # if type(obj).__name__ == 'DeviceArray':
        #     data = nghbr_dis
        #     # TRUE:    Î¼=1.010   ÏƒMax=0.119      mean=1.004      sigma=0.098     sigma_tmp=0.098     std=0.098   obj=-1.8202
        #     # oldTRUE: Î¼=0.985     mean=1.004      sigma=0.098     std=0.098   obj=-1.8019
        #     # BCC1e1:         rmsd_intra=3.72   disterr_intra=39.9   disterr_interhmlg=70.8   ndv_nrmse=3.53  ()
        #     # norm_dis:       rmsd_intra=   disterr_intra=   disterr_interhmlg=   ndv_nrmse= ()
        #     # norm_dis_nomin: rmsd_intra=   disterr_intra=   disterr_interhmlg=   ndv_nrmse= ()
        #     print(f'Î¼={mu:.3f} \t ÏƒMax={sigma_max:.3f} \t mean={data.mean():.3f} \t sigma={sigma:.3f} \t sigma_tmp={sigma_tmp:.3f} \t std={data.std():.3f} \t obj={obj:.5g}')

        if not ag_np.isfinite(obj):
            raise ValueError(f"{self.name} constraint objective is {obj}.")
        return self.lambda_val * obj


class BeadChainConnectivity2022(Constraint):
    """TODO"""

    def __init__(self, lambda_val, lengths, ploidy, multiscale_factor=1,
                 hparams=None, fullres_struct_nan=None, lowmem=False, mods=[]):
        self.abbrev = "bcc"
        self.name = "Bead-chain connectivity (2022)"
        self.during_alpha_infer = True
        self.lambda_val = lambda_val
        self.lengths = np.asarray(lengths)
        self.lengths_lowres = decrease_lengths_res(
            self.lengths, multiscale_factor=multiscale_factor)
        self.ploidy = ploidy
        self.multiscale_factor = multiscale_factor
        self.hparams = hparams
        self._fullres_struct_nan = None  # Not necessary for this constraint
        self._lowmem = lowmem
        self._var = None
        self.mods = mods  # TODO remove

        self.check()

    def check(self):
        if self.ploidy == 1 and self.lambda_val > 0:
            raise ValueError(f"{self.name} constraint can not be applied to"
                             " haploid genomes.")
        if self.lambda_val < 0:
            raise ValueError("Constraint lambda may not be < 0.")
        if self.hparams is None or 'counts_inter_mv' not in self.hparams or (
                self.hparams['counts_inter_mv'] is None):
            raise ValueError(f"{self.name} constraint is missing"
                             f" neccessary hyperparams: 'counts_inter_mv'")

    def _setup(self, counts=None, bias=None):
        if self.lambda_val <= 0:
            return
        if self._var is not None:
            return self._var

        # Beta
        beta = _ambiguate_beta(
            [c.beta for c in counts if c.sum() != 0], counts=counts,
            lengths=self.lengths, ploidy=self.ploidy)

        # Number of fullres beads per lowres bead
        if self.multiscale_factor > 1:
            fullres_per_lowres_bead = _count_fullres_per_lowres_bead(
                multiscale_factor=self.multiscale_factor, lengths=self.lengths,
                ploidy=self.ploidy, fullres_struct_nan=self._fullres_struct_nan)
        else:
            fullres_per_lowres_bead = None

        # Get indices of neighboring beads
        row_nghbr = _neighboring_bead_indices(
            lengths=self.lengths, ploidy=self.ploidy,
            multiscale_factor=self.multiscale_factor)

        # Get bias corresponding to neighboring beads
        row_nghbr_ambig_fullres = _neighboring_bead_indices(
            lengths=self.lengths, ploidy=1, multiscale_factor=1)
        if bias is None or np.all(bias == 1):
            bias_nghbr = 1
        else:
            bias_nghbr = bias.ravel()[row_nghbr_ambig_fullres] * bias.ravel()[
                row_nghbr_ambig_fullres + 1]

        # Get counts corresponding to neighboring beads
        # counts_ambig = ambiguate_counts(
        #     counts=counts, lengths=self.lengths, ploidy=self.ploidy,
        #     exclude_zeros=False)  # FIXME
        if self.multiscale_factor == 1:
            counts_ambig = ambiguate_counts(
                counts=counts, lengths=self.lengths, ploidy=self.ploidy,
                exclude_zeros=False)  # FIXME
            counts_nghbr = counts_ambig[
                row_nghbr_ambig_fullres, row_nghbr_ambig_fullres + 1]
            counts_nghbr[np.isnan(counts_nghbr)] = np.nanmean(counts_nghbr)
            counts_nghbr_mask = None
        else:
            if len(counts) > 2:
                raise NotImplementedError("Make .add and .ambiguate methods for counts object")
            if not (bias is None or np.all(bias == 1)):
                raise NotImplementedError("Need to include zero counts in ambiguated and also get nghbr counts mask")

            counts_ambig = [c for c in counts if c.sum() != 0][0]  # FIXME
            # counts_nghbr = _counts_near_diag(
            #     counts_ambig, self.lengths, ploidy=self.ploidy, nbins=1,
            #     exclude_zeros=True)
            # tmp = _group_counts_multiscale(
            #     counts_nghbr, lengths=self.lengths, ploidy=self.ploidy,
            #     multiscale_factor=self.multiscale_factor)
            # counts_nghbr, indices, indices3d, counts_shape, counts_mask = tmp

            row_nghbr_ambig_lowres = _neighboring_bead_indices(
                lengths=self.lengths, ploidy=1,
                multiscale_factor=self.multiscale_factor)
            mask = (counts_ambig.col == counts_ambig.row + 1) & np.isin(
                counts_ambig.row, row_nghbr_ambig_lowres)
            row_nghbr_counts = counts_ambig.row[mask]

            if not np.array_equal(row_nghbr_counts, row_nghbr_ambig_lowres):
                raise NotImplementedError("double check this code")
                counts_nghbr = np.zeros(row_nghbr_ambig_lowres.shape)
                counts_nghbr[:, row_nghbr_counts] = counts_ambig.data[:, mask]
                tmp = ~np.isin(row_nghbr_ambig_lowres, row_nghbr_counts)
                counts_nghbr[:, tmp] = np.mean(
                    counts_nghbr[:, row_nghbr_counts])
            else:
                counts_nghbr = counts_ambig.data[:, mask]

            counts_nghbr_mask = None  # FIXME

        var = {
            'row_nghbr': row_nghbr, 'counts_nghbr': counts_nghbr,
            'bias_nghbr': bias_nghbr, 'beta': beta,
            'fullres_per_lowres_bead': fullres_per_lowres_bead,
            'counts_nghbr_mask': counts_nghbr_mask}
        if not self._lowmem:
            self._var = var
        return var

    def apply(self, struct, alpha=None, epsilon=None, counts=None, bias=None,
              inferring_alpha=False):
        if self.lambda_val == 0 or (
                inferring_alpha and not self.during_alpha_infer):
            return 0.
        elif alpha is None:
            raise ValueError(f"Must input alpha for {self.name} constraint.")
        var = self._setup(counts=counts, bias=bias)

        nghbr_dis = _euclidean_distance(
            struct, row=var['row_nghbr'], col=var['row_nghbr'] + 1)

        counts_inter_mean = self.hparams['counts_inter_mv'][1]['mean']
        lambda_interchrom = counts_inter_mean / 2

        if self.multiscale_factor == 1:
            lambda_nghbr = (2 * var['beta']) * var['bias_nghbr'] * ag_np.power(
                nghbr_dis, alpha)
            lambda_nghbr = lambda_nghbr + lambda_interchrom
            obj = poisson_nll(
                np.tile(var['counts_nghbr'], 2), lambda_pois=lambda_nghbr)
        else:
            gamma_mean, gamma_var = get_gamma_moments(
                dis=nghbr_dis, epsilon=epsilon, alpha=alpha,
                beta=(2 * var['beta']), ambiguity='ua')

            # Add lambda_interchrom (increases gamma mean & Poisson variance)
            # FIXME TODO we want fullres lambda_interchrom, right?
            gamma_mean = gamma_mean + (
                lambda_interchrom / np.square(self.multiscale_factor))  # TODO this is the best option, right?

            theta = gamma_var / gamma_mean
            k = ag_np.square(gamma_mean) / gamma_var

            num_fullres_per_lowres_bins = (
                var['fullres_per_lowres_bead'][var['row_nghbr']]) * (
                var['fullres_per_lowres_bead'][var['row_nghbr'] + 1])

            obj = gamma_poisson_nll(
                theta=theta, k=k, data=np.tile(var['counts_nghbr'], 2),
                bias=var['bias_nghbr'], mask=var['counts_nghbr_mask'],
                num_fullres_per_lowres_bins=num_fullres_per_lowres_bins,
                mods=self.mods)

            lambda_nghbr = gamma_mean  # TODO temp

        if False and 'debug' in self.mods and type(obj).__name__ in ('DeviceArray', 'ndarray'):
            to_print = f"ð”¼[c]={var['counts_nghbr'].mean():.3g}\t   Î¼NB={lambda_nghbr.mean():.2g}"
            if epsilon is not None:
                to_print += f"\t   V[c]={var['counts_nghbr'].var(axis=0).mean():.3g}"
                to_print += f"\t   ÏƒÂ²NB={(gamma_mean + gamma_var).mean():.3g}\t   Îµ={epsilon:.2g}"
                # lambda_nghbr = (2 * var['beta']) * var['bias_nghbr'] * ag_np.power(
                #     nghbr_dis, alpha) + lambda_interchrom
                # to_print += f"\t   Î¼*={lambda_nghbr.mean():.2g}"
            print(to_print + f"\t   obj={obj:.3g}", flush=True)
            # exit(1)

        if not ag_np.isfinite(obj):
            raise ValueError(f"{self.name} constraint objective is {obj}.")
        return self.lambda_val * obj


class HomologSeparating2022(Constraint):
    """TODO"""

    def __init__(self, lambda_val, lengths, ploidy, multiscale_factor=1,
                 hparams=None, fullres_struct_nan=None, lowmem=False, mods=[]):
        self.abbrev = "hsc"
        self.name = "Homolog separating (2022)"
        self.during_alpha_infer = True
        self.lambda_val = lambda_val
        self.lengths = np.asarray(lengths)
        self.lengths_lowres = decrease_lengths_res(
            self.lengths, multiscale_factor=multiscale_factor)
        self.ploidy = ploidy
        self.multiscale_factor = multiscale_factor
        self.hparams = hparams
        self._fullres_struct_nan = fullres_struct_nan  # For self._setup() only
        self._lowmem = lowmem
        self._var = None
        self.mods = mods  # TODO remove

        self.check()

    def check(self):
        if self.ploidy == 1 and self.lambda_val > 0:
            raise ValueError(f"{self.name} constraint can not be applied to"
                             " haploid genomes.")
        if self.lambda_val < 0:
            raise ValueError("Constraint lambda may not be < 0.")
        # Hyperparam: perc_diff  # TODO maybe remove
        if self.hparams is None:
            self.hparams = {'perc_diff': None}
        if 'perc_diff' not in self.hparams:
            self.hparams['perc_diff'] = None
        if self.hparams['perc_diff'] is not None and (
                self.hparams['perc_diff'] < 0 or self.hparams['perc_diff'] > 1):
            raise ValueError("'perc_diff' must be between 0 and 1.")
        # Hyperparam: counts_inter_mv
        if self.hparams is None or 'counts_inter_mv' not in self.hparams or (
                self.hparams['counts_inter_mv'] is None):
            raise ValueError(f"{self.name} constraint is missing"
                             f" neccessary hyperparams: 'counts_inter_mv'")

    def _setup(self, counts=None, bias=None):
        if self.lambda_val <= 0:
            return
        if self._var is not None:
            return self._var

        beta = _ambiguate_beta(
            [c.beta for c in counts if c.sum() != 0], counts=counts,
            lengths=self.lengths, ploidy=self.ploidy)

        if self.multiscale_factor > 1:
            fullres_per_lowres_bead = _count_fullres_per_lowres_bead(
                multiscale_factor=self.multiscale_factor, lengths=self.lengths,
                ploidy=self.ploidy, fullres_struct_nan=self._fullres_struct_nan)
        else:
            fullres_per_lowres_bead = None

        if self.lengths_lowres.size > 1 and 'interhmlg_intrachrom' in self.mods:
            n = self.lengths_lowres.sum()
            mask_intrachrom = _intra_mask(
                (n, n), lengths_at_res=self.lengths_lowres)
        else:
            mask_intrachrom = None

        var = {'beta': beta, 'fullres_per_lowres_bead': fullres_per_lowres_bead,
               'mask_intrachrom': mask_intrachrom}
        if not self._lowmem:
            self._var = var
            self._fullres_struct_nan = None  # No longer needed unless lowmem
        return var

    def apply(self, struct, alpha=None, epsilon=None, counts=None, bias=None,
              inferring_alpha=False):
        if self.lambda_val == 0 or (
                inferring_alpha and not self.during_alpha_infer):
            return 0.
        elif alpha is None:
            raise ValueError(f"Must input alpha for {self.name} constraint.")
        var = self._setup(counts=counts, bias=bias)

        # Get inter-homolog distances
        n = self.lengths_lowres.sum()
        row, col = (x.flatten() for x in np.indices((n, n)))
        if 'same_locus' in self.mods:
            mask = row == col
            row = row[mask]
            col = col[mask]
        elif 'interhmlg_intrachrom' in self.mods and 'mean' not in self.mods:
            row = row[var['mask_intrachrom']]
            col = col[var['mask_intrachrom']]
            # exit(0)
        dis_interhmlg = _euclidean_distance(struct, row=row, col=col + n)

        counts_inter_mv = self.hparams['counts_inter_mv'][self.multiscale_factor]
        counts_inter_mean = counts_inter_mv['mean']

        if 'mse' in self.mods:
            if 'use_gmean' in self.mods and self.multiscale_factor > 1:
                lambda_interhmlg, _ = get_gamma_moments(
                    dis=dis_interhmlg, epsilon=epsilon, alpha=alpha,
                    beta=4 * var['beta'], ambiguity='ua')  # FIXME
            else:
                dis_alpha_interhmlg = ag_np.power(dis_interhmlg, alpha)
                lambda_interhmlg = (4 * var['beta']) * dis_alpha_interhmlg
            lambda_interhmlg = lambda_interhmlg * np.square(self.multiscale_factor)  # TODO this is the best option, right?
            if 'mean' in self.mods:
                lambda_interhmlg = lambda_interhmlg.reshape(n, n)
                nchrom = self.lengths.size
                interhmlg_mean = ag_np.zeros((nchrom, nchrom))
                begin_i = end_i = 0
                for i in range(nchrom):
                    end_i = end_i + ag_np.int32(self.lengths_lowres[i])
                    begin_j = end_j = 0
                    for j in range(nchrom):
                        end_j = end_j + ag_np.int32(self.lengths_lowres[j])
                        if 'interhmlg_intrachrom' in self.mods and i != j:
                            begin_j = end_j
                            continue
                        # tmp = ag_np.mean(
                        #     lambda_interhmlg[begin_i:end_i, begin_j:end_j])
                        # print(f"i={i}=[{begin_i}:{end_i}],  j={j}=[{begin_j}:{end_j}],   {tmp}")
                        interhmlg_mean = interhmlg_mean.at[i, j].set(ag_np.mean(
                            lambda_interhmlg[begin_i:end_i, begin_j:end_j]))
                        begin_j = end_j
                    begin_i = end_i
                lambda_interhmlg = interhmlg_mean
                if 'interhmlg_intrachrom' in self.mods:
                    lambda_interhmlg = lambda_interhmlg[lambda_interhmlg != 0]
            if 'flex' in self.mods:
                obj = _mse_flexible(
                    actual=lambda_interhmlg, expected=counts_inter_mean,
                    cutoff=self.hparams['perc_diff'], scale_by_expected=True)
            else:
                obj = _mse_outside_of_window(
                    actual=lambda_interhmlg, expected=counts_inter_mean,
                    cutoff=self.hparams['perc_diff'], scale_by_expected=True)
            if 'debug' in self.mods and type(obj).__name__ in ('DeviceArray', 'ndarray'):
                tmp = f'\t  ÏƒÂ²={lambda_interhmlg.var():.2g}'
                if 'mean' in self.mods:
                    tmp = tmp + '\t  Î»=' + ', '.join([f"{x:.2g}" for x in lambda_interhmlg._value.ravel().tolist()])
                if epsilon is None:
                    print(f"c={counts_inter_mean:.3g}\tÂµ={lambda_interhmlg.mean():.3g}\t  obj={obj:.2g}{tmp}", flush=True)
                else:
                    print(f"c={counts_inter_mean:.3g}\tÂµ={lambda_interhmlg.mean():.3g}\t  obj={obj:.2g}\t  Îµ={epsilon:.2g}{tmp}", flush=True)
        elif ('hsc_gamma' in self.mods) or ('hsc_invgamma' in self.mods) or ('hsc_normal' in self.mods):
            # NOTE: DON'T MULTIPLY lambda_interhmlg BY square(multiscale_factor)... and use highres counts_inter mean & var for all
            if 'div4' in self.mods or 'try1' in self.mods:
                lambda_interhmlg = var['beta'] * ag_np.power(dis_interhmlg, alpha)
            else:
                lambda_interhmlg = (4 * var['beta']) * ag_np.power(dis_interhmlg, alpha)

            counts_inter_mv = self.hparams['counts_inter_mv'][1]
            mean = counts_inter_mv['est_gamma_mean']
            variance = counts_inter_mv['est_gamma_var']

            # variance = lambda_interhmlg.variance()
            # variance = relu_max(variance, counts_inter_mv['gamma_var_min'])
            # variance = relu_min(variance, counts_inter_mv['gamma_var_max'])
            # theta = variance / counts_inter_mean
            # k = counts_inter_mean / est_gamma_theta

            k = counts_inter_mv['est_gamma_k']
            theta = counts_inter_mv['est_gamma_theta']

            if 'hsc_invgamma' in self.mods:
                if 'div4' in self.mods:
                    div_by = var['beta']
                else:
                    div_by = 4 * var['beta']
                k, theta = gamma_to_invgamma_to_gamma(
                    k=k, theta=theta, scale=1 / div_by, verbose=False)
                obj = -logpdf_gamma(
                    ag_np.power(dis_interhmlg, -alpha), a=k, scale=theta).mean()
            elif 'hsc_normal' in self.mods:
                obj = -logpdf_norm(
                    lambda_interhmlg, loc=mean, scale=np.sqrt(variance)).mean()
            else:
                obj = -logpdf_gamma(lambda_interhmlg, a=k, scale=theta).mean()


            # FIXME FIXME2
            if 'debug' in self.mods and type(obj).__name__ in ('DeviceArray', 'ndarray'):
                # \t   Var[c]-ð”¼[c]={counts_inter_mv['var'] - counts_inter_mean:.2g}
                to_print = f"ð”¼[c]={mean:.2g}\t   Î¼={lambda_interhmlg.mean():.2g}\t   var={variance:.2g}\t   ÏƒÂ²={lambda_interhmlg.var():.2g}"

                if 'hsc_invgamma' in self.mods:
                    mean2 = k * theta
                    var2 = k * (theta ** 2)
                    lam2 = ag_np.power(dis_interhmlg, -alpha)
                    to_print += f"\t   ... ð”¼[c]={mean2:.2g}\t   Î¼={lam2.mean():.2g}\t   var={var2:.2g}\t   ÏƒÂ²={lam2.var():.2g}"

                # k = ag_np.square(lambda_interhmlg.mean()) / lambda_interhmlg.var()
                # theta = lambda_interhmlg.var() / lambda_interhmlg.mean()
                # to_print += f"\t   k*={counts_inter_mv['est_gamma_k']:.2g}\t   k={k:.2g}\t   Î¸*={counts_inter_mv['est_gamma_theta']:.2g}\t   Î¸={theta:.2g}"

                to_print += f"\t   OBJ={obj:.2g}"
                if epsilon is not None:
                    to_print += f"\t   Îµ={epsilon:.2g}"
                print(to_print, flush=True)
                # exit(1)
        else:
            counts_inter_mv = self.hparams['counts_inter_mv'][1]
            counts_inter_mean = counts_inter_mv['mean']

            dis_alpha_interhmlg = ag_np.power(dis_interhmlg, alpha)
            lambda_interhmlg = (4 * var['beta']) * dis_alpha_interhmlg
            # lambda_interhmlg = lambda_interhmlg * np.square(self.multiscale_factor)  # TODO this is the best option, right?
            obj = poisson_nll(counts_inter_mean, lambda_pois=lambda_interhmlg)

            if 'debug' in self.mods and type(obj).__name__ in ('DeviceArray', 'ndarray'):
                to_print = f"c={counts_inter_mean:.2g}\t   Î¼={lambda_interhmlg.mean():.2g}\t   obj={obj:.2g}"
                if epsilon is not None:
                    to_print += f"\t   Îµ={epsilon:.2g}"
                print(to_print, flush=True)

        if not ag_np.isfinite(obj):
            raise ValueError(f"{self.name} constraint objective is {obj}.")
        return self.lambda_val * obj


def _mse_flexible(actual, expected, cutoff=None, scale_by_expected=True):
    """TODO"""

    if scale_by_expected:
        actual = actual / expected
        expected = 1

    if cutoff is not None and cutoff != 0:
        if scale_by_expected:
            window = cutoff
        else:
            window = cutoff * expected

        mle_expected = ag_np.mean(actual)

        # print(f"{window=:g}")
        # print(f"{expected=:g}")
        # print(f"{mle_expected=:g}")

        sub_from_expected = relu_min(relu(expected - mle_expected), window)
        add_to_expected = relu_min(relu(mle_expected - expected), window)
        expected = expected + add_to_expected - sub_from_expected

        # print(f"{add_to_expected=:g}")
        # print(f"{sub_from_expected=:g}")
        # print(f"{expected=:g}")

    diff = expected - actual
    mse = ag_np.mean(ag_np.square(diff))

    return mse


def _mse_outside_of_window(actual, expected, cutoff=None, scale_by_expected=True, new=True):
    """TODO"""
    if scale_by_expected:
        diff = 1 - actual / expected
    else:
        diff = expected - actual

    if cutoff is None:
        diff = relu(diff)
        mse = ag_np.mean(ag_np.square(diff))
        raise ValueError("I thought we weren't doing ReLU for HSC anymore")  # TODO remove ValueError
        return mse

    if cutoff == 0:
        mse = ag_np.mean(ag_np.square(diff))
        return mse

    if scale_by_expected:
        window = cutoff
    else:
        window = cutoff * expected

    if new:
        n = diff.size
        diff_gt0 = relu(diff)
        diff_lt0 = -relu(-diff)
        diff_gt0 = relu(diff_gt0 - window)
        diff_lt0 = -relu(-(diff_lt0 + window))
        mse = (ag_np.square(diff_gt0).sum() + ag_np.square(diff_lt0).sum()) / n
    else:  # TODO remove old code
        window = ag_np.array(window)
        if window.size == 1 and actual.size > 1:
            window = ag_np.tile(window, actual.size)
        gt0 = diff > 0
        diff = diff.at[gt0].set(relu(diff[gt0] - window[gt0]))
        diff = diff.at[~gt0].set(-relu(-(diff[~gt0] + window[~gt0])))
        mse = ag_np.mean(ag_np.square(diff))

    return mse


def prep_constraints(counts, lengths, ploidy, multiscale_factor=1,
                     bcc_lambda=0, hsc_lambda=0, bcc_version='2019',
                     hsc_version='2019', counts_inter_mv=None,
                     est_hmlg_sep=None, hsc_perc_diff=None,
                     fullres_struct_nan=None, verbose=True, mods=[]):
    """TODO"""

    # TODO remove
    if mods is None:
        mods = []
    elif isinstance(mods, str):
        mods = mods.lower().split('.')
    else:
        mods = [x.lower() for x in mods]

    if bcc_version is None:
        bcc_version = '2019'
    bcc_version = str(bcc_version)
    if hsc_version is None:
        hsc_version = '2019'
    hsc_version = str(hsc_version)

    bcc_class = {
        '2019': BeadChainConnectivity2019, '2021': BeadChainConnectivity2021,
        '2022': BeadChainConnectivity2022}
    hsc_class = {
        '2019': HomologSeparating2019, '2022': HomologSeparating2022}
    bcc_hparams = {
        '2019': None, '2021': None,
        '2022': {'counts_inter_mv': counts_inter_mv}}
    hsc_hparams = {
        '2019': {'est_hmlg_sep': est_hmlg_sep, 'perc_diff': hsc_perc_diff},
        '2022': {'counts_inter_mv': counts_inter_mv,
                 'perc_diff': hsc_perc_diff}}

    # _ = get_counts_interchrom(
    #     counts=counts, lengths=lengths, ploidy=ploidy,
    #     multiscale_factor=multiscale_factor,
    #     fullres_struct_nan=fullres_struct_nan, verbose=True)

    constraints = []
    if bcc_lambda != 0:
        constraints.append(bcc_class[bcc_version](
            bcc_lambda, lengths=lengths, ploidy=ploidy,
            multiscale_factor=multiscale_factor,
            hparams=bcc_hparams[bcc_version],
            fullres_struct_nan=fullres_struct_nan, lowmem=False, mods=mods))
    if hsc_lambda != 0:
        constraints.append(hsc_class[hsc_version](
            hsc_lambda, lengths=lengths, ploidy=ploidy,
            multiscale_factor=multiscale_factor,
            hparams=hsc_hparams[hsc_version],
            fullres_struct_nan=fullres_struct_nan, lowmem=False, mods=mods))

    # TODO have var=None input to Constraint._setup = share var dict for all (only add to var if not already found)
    # TODO if NOT lowmem, run Constraint._setup method here = share var dict for all

    # TODO have var=None input to Constraint.apply = share var dict for all during opt if lowmem
    # TODO if lowmem, run Constraint._setup before .apply during opt = share var dict for all during opt if lowmem

    return constraints


def get_fullres_counts_interchrom(counts, lengths, ploidy, verbose=False):
    """TODO"""  # TODO remove function?

    if lengths.size == 1:
        raise ValueError(
            "Must input counts_interchrom if inferring single chromosome.")
    counts_ambig = ambiguate_counts(
        counts=counts, lengths=lengths, ploidy=ploidy, exclude_zeros=False)
    counts_interchrom = _inter_counts(
        counts_ambig, lengths, ploidy=ploidy, exclude_zeros=False)
    counts_interchrom = counts_interchrom[~np.isnan(counts_interchrom)]
    if verbose:
        print(f"Inter-chromosomal counts Î¼={counts_interchrom.mean():.3g}"
              f"  ÏƒÂ²={counts_interchrom.var():.3g}", flush=True)
    return counts_interchrom


def get_counts_interchrom(counts, lengths, ploidy, multiscale_factor=1,
                          fullres_struct_nan=None, verbose=False):
    """TODO"""

    if lengths.size == 1:
        raise ValueError(
            "Must input counts_interchrom if inferring single chromosome.")
    lengths_lowres = decrease_lengths_res(
        lengths, multiscale_factor=multiscale_factor)

    counts_ambig = ambiguate_counts(
        counts=counts, lengths=lengths_lowres, ploidy=ploidy,
        exclude_zeros=False)
    counts_interchrom = _inter_counts(
        counts_ambig, lengths_at_res=lengths_lowres, ploidy=ploidy,
        exclude_zeros=False)
    if multiscale_factor > 1:
        fullres_per_lowres_bead = _count_fullres_per_lowres_bead(
            multiscale_factor=multiscale_factor, lengths=lengths,
            ploidy=1, fullres_struct_nan=fullres_struct_nan)
        mask = fullres_per_lowres_bead == multiscale_factor
        counts_interchrom[~mask, :] = np.nan
        counts_interchrom[:, ~mask] = np.nan

    counts_interchrom = counts_interchrom[~np.isnan(counts_interchrom)]
    if verbose:
        print(f"Inter-chromosomal counts Î¼={counts_interchrom.mean():.3g}"
              f"  ÏƒÂ²={counts_interchrom.var():.3g}", flush=True)
    return counts_interchrom


# def gamma_to_gengamma_to_gamma(k, theta, q, scale=1):
#     theta = theta * scale
#     p = 1 / q
#     d = k / q
#     a = ag_np.power(theta, q)


def gamma_to_invgamma_to_gamma(k, theta, scale=1, verbose=False):  # FIXME FIXME2
    # Scale original gamma distribution
    theta = theta * scale
    # Get InvGamma params
    a = k
    b = 1 / theta
    # Get InvGamma moments
    mean = b / (a - 1)
    var = (mean ** 2) / (a - 2)
    # Get params of new gamma distribution
    theta_new = var / mean
    k_new = (mean ** 2) / var
    if verbose:
        print(f"OLD: Î¼={k * theta:.2g}\t    ÏƒÂ²={k * (theta ** 2):.2g}", flush=True)
        print(f"NEW: Î¼={mean:.2g}\t    ÏƒÂ²={var:.2g}", flush=True)
    return k_new, theta_new


def calc_counts_interchrom(counts, lengths, ploidy, multiscale_rounds=1,
                           filter_threshold=0.04, normalize=True, bias=None,
                           verbose=True, mods=[]):
    """TODO"""

    counts, bias, lengths, _, _, _, _ = load_data(
        counts=counts, lengths_full=lengths, ploidy=ploidy,
        filter_threshold=filter_threshold, normalize=normalize, bias=bias,
        exclude_zeros=False, verbose=False)
    if lengths.size == 1:
        raise ValueError(
            "Must input counts_interchrom if inferring a single chromosome.")
    if bias is not None and bias.size != lengths.sum() * ploidy:
        raise ValueError("Length of bias vector does not match the counts")

    # Get normalized inter-chromosomal counts
    counts_ambig = ambiguate_counts(
        counts=counts, lengths=lengths, ploidy=ploidy, exclude_zeros=False)
    counts_interchrom = _inter_counts(
        counts_ambig, lengths, ploidy=ploidy, exclude_zeros=False)
    if bias is not None:
        counts_ambig /= bias.reshape(1, -1)
        counts_ambig /= bias.reshape(-1, 1)

    # Get mean & var of full-res normalized inter-chromosomal counts
    counts_inter_mv = {}
    tmp = counts_interchrom[~np.isnan(counts_interchrom)]
    if 'try1' in mods:
        tmp = tmp / 4
    counts_inter_mv[1] = {'mean': tmp.mean(), 'var': tmp.var()}

    # Get mean & var of low-res normalized inter-chromosomal counts
    lengths_lowres = lengths
    if multiscale_rounds > 1:
        fullres_struct_nan = find_beads_to_remove(
            counts_ambig, lengths=lengths, ploidy=1, multiscale_factor=1)
    all_multiscale_factors = 2 ** np.arange(multiscale_rounds)
    for multiscale_factor in all_multiscale_factors[1:]:

        counts_interchrom = decrease_counts_res(
            counts_interchrom, multiscale_factor=2, lengths=lengths_lowres,
            ploidy=ploidy)
        lengths_lowres = decrease_lengths_res(
            lengths_lowres, multiscale_factor=2)

        # tmp = counts_interchrom.copy()
        counts_interchrom = _inter_counts(
            counts_interchrom, lengths_at_res=lengths_lowres, ploidy=ploidy,
            exclude_zeros=False)
        # print((counts_interchrom != tmp).sum())

        fullres_per_lowres_bead = _count_fullres_per_lowres_bead(
            multiscale_factor=multiscale_factor, lengths=lengths,
            ploidy=1, fullres_struct_nan=fullres_struct_nan)
        mask = (fullres_per_lowres_bead == multiscale_factor)
        counts_interchrom[~mask, :] = np.nan
        counts_interchrom[:, ~mask] = np.nan

        tmp = counts_interchrom[~np.isnan(counts_interchrom)]
        if 'try1' in mods:
            tmp = tmp / 4
        counts_inter_mv[multiscale_factor] = {
            'mean': tmp.mean(), 'var': tmp.var()}

    # Get estimated gamma theta & k for hsc2022
    tmp_factor = 0.01
    for multiscale_factor in all_multiscale_factors:
        mean = counts_inter_mv[multiscale_factor]['mean']
        var = counts_inter_mv[multiscale_factor]['var']

        gamma_var = var - mean
        gamma_mean = mean
        if gamma_var <= 0:
            gamma_var = tmp_factor * gamma_mean

        if 'div4' in mods:
            gamma_var = gamma_var / np.square(4)
            gamma_mean = gamma_mean / 4

        gamma_theta = gamma_var / gamma_mean
        gamma_k = np.square(gamma_mean) / gamma_var

        counts_inter_mv[multiscale_factor]['est_gamma_mean'] = gamma_mean
        counts_inter_mv[multiscale_factor]['est_gamma_var'] = gamma_var
        counts_inter_mv[multiscale_factor]['est_gamma_theta'] = gamma_theta
        counts_inter_mv[multiscale_factor]['est_gamma_k'] = gamma_k

        # tmp = np.square(mean) / var + 1
        # counts_inter_mv[multiscale_factor]['est_invgamma_a'] = tmp + 1
        # counts_inter_mv[multiscale_factor]['est_invgamma_b'] = tmp * mean

        gamma_var_min = gamma_var
        gamma_var_max = np.mean([mean, gamma_var_min])
        counts_inter_mv[multiscale_factor]['gamma_var_min'] = gamma_var_min
        counts_inter_mv[multiscale_factor]['gamma_var_max'] = gamma_var_max

    if verbose:
        for multiscale_factor in all_multiscale_factors:
            mean = counts_inter_mv[multiscale_factor]['mean']
            var = counts_inter_mv[multiscale_factor]['var']
            gamma_var = counts_inter_mv[multiscale_factor]['est_gamma_var']
            theta = counts_inter_mv[multiscale_factor]['est_gamma_theta']
            print(f"INTER-CHROM COUNTS, {multiscale_factor}X:\t  "
                  f"mean={mean:.3g}\tvar={var:.3g}\tÎ¸={theta:.3g}"
                  f"\tÎ“var={gamma_var:.3g}", flush=True)

    return counts_inter_mv


def taylor_approx_ndc(x, beta=1, alpha=-3, order=1):
    x = x / beta
    x_mean = np.mean(x)
    x_var = np.var(x)

    fx_mean = np.power(x_mean, 1 / alpha)
    fx_var = 1 / np.power(alpha, 2) * np.power(
        x_mean, (2 - 2 * alpha) / alpha) * x_var

    if order == 2:  # FIXME TODO
        tmp = (1 - alpha) / (2 * np.square(alpha)) * np.power(
            x_mean, (1 - 2 * alpha) / alpha)
        fx_mean = fx_mean + tmp * x_var
        fx_var = fx_var + np.square(tmp) * (np.var(
            np.square(x)) - 4 * np.square(x_mean) * x_var)

    fx_std = np.sqrt(fx_var)
    return fx_mean, fx_std


def _neighboring_bead_indices(lengths, ploidy, multiscale_factor=1,
                              counts=None, include_struct_nan_beads=True):
    """Return row & col of neighboring beads, along a homolog of a chromosome.
    """

    lengths_lowres = decrease_lengths_res(lengths, multiscale_factor)
    nbeads = lengths_lowres.sum() * ploidy

    row_nghbr = np.arange(nbeads - 1, dtype=int)

    # Optionally remove beads for which there is no counts data
    if not include_struct_nan_beads:
        if counts is None:
            raise ValueError(
                "Counts must be inputted if including struct_nan beads.")
        struct_nan = find_beads_to_remove(
            counts, lengths=lengths, ploidy=ploidy,
            multiscale_factor=multiscale_factor)
        nghbr_dis_mask = (~np.isin(row_nghbr, struct_nan)) & (
            ~np.isin(row_nghbr + 1, struct_nan))
        row_nghbr = row_nghbr[nghbr_dis_mask]

    # Remove if "neighbor" beads are actually on different chromosomes
    # or homologs
    bins = np.tile(lengths_lowres, ploidy).cumsum()
    same_bin = np.digitize(row_nghbr, bins) == np.digitize(row_nghbr + 1, bins)

    row_nghbr = row_nghbr[same_bin]

    return row_nghbr


def _inter_homolog_dis(struct, lengths):
    """Computes distance between homologs for a normal diploid structure.
    """

    struct = struct.copy().reshape(-1, 3)

    n = int(struct.shape[0] / 2)
    homo1 = struct[:n, :]
    homo2 = struct[n:, :]

    hmlg_dis = []
    begin = end = 0
    for i in range(lengths.size):
        end += lengths[i]
        if np.isnan(homo1[begin:end, 0]).sum() == lengths[i] or np.isnan(
                homo2[begin:end, 0]).sum() == lengths[i]:
            hmlg_dis.append(np.nan)
        else:
            hmlg_dis.append(((np.nanmean(homo1[
                begin:end, :], axis=0) - np.nanmean(
                homo2[begin:end, :], axis=0)) ** 2).sum() ** 0.5)
        begin = end

    hmlg_dis = np.array(hmlg_dis)
    hmlg_dis[np.isnan(hmlg_dis)] = np.nanmean(hmlg_dis)

    return hmlg_dis


def _inter_homolog_dis_via_simple_diploid(struct, lengths):
    """Computes distance between chromosomes for a faux-haploid structure.
    """

    from sklearn.metrics import euclidean_distances

    struct = struct.copy().reshape(-1, 3)

    chrom_barycenters = []
    begin = end = 0
    for i in range(lengths.size):
        end += lengths[i]
        if np.isnan(struct[begin:end, 0]).sum() < lengths[i]:
            chrom_barycenters.append(
                np.nanmean(struct[begin:end, :], axis=0).reshape(1, 3))
        begin = end

    chrom_barycenters = np.concatenate(chrom_barycenters)

    hmlg_dis = euclidean_distances(chrom_barycenters)
    hmlg_dis[np.tril_indices(hmlg_dis.shape[0])] = np.nan

    return np.full(lengths.shape, np.nanmean(hmlg_dis))


def distance_between_homologs(structures, lengths, mixture_coefs=None,
                              simple_diploid=False):
    """Computes distances between homologs for a given structure.

    For diploid organisms, this computes the distance between homolog centers
    of mass for each chromosome.

    Parameters
    ----------
    structures : array of float or list of array of float
        3D chromatin structure(s) for which to assess inter-homolog distances.
    lengths : array_like of int
        Number of beads per homolog of each chromosome.
    simple_diploid: bool, optional
        For diploid organisms: whether the structure is an inferred "simple
        diploid" structure in which homologs are assumed to be identical and
        completely overlapping with one another.

    Returns
    -------
    array of float
        Distance between homologs per chromosome.

    """

    from .utils_poisson import _format_structures

    structures = _format_structures(
        structures=structures, lengths=lengths,
        ploidy=(1 if simple_diploid else 2),
        mixture_coefs=mixture_coefs)

    hmlg_dis = []
    for struct in structures:
        if simple_diploid:
            hmlg_dis.append(_inter_homolog_dis_via_simple_diploid(
                struct=struct, lengths=lengths))
        else:
            hmlg_dis.append(_inter_homolog_dis(struct=struct, lengths=lengths))

    return np.mean(hmlg_dis, axis=0)
