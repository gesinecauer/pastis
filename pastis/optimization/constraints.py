import sys

if sys.version_info[0] < 3:
    raise Exception("Must be using Python 3")

import numpy as np
import re

from absl import logging as absl_logging
absl_logging.set_verbosity('error')
from jax.config import config as jax_config
jax_config.update("jax_platform_name", "cpu")
jax_config.update("jax_enable_x64", True)
import jax.numpy as ag_np
from jax.nn import relu
from jax.scipy.stats.nbinom import logpmf as logpmf_negbinom
from jax.scipy.stats.poisson import logpmf as logpmf_poisson
from jax.scipy.stats.gamma import logpdf as logpdf_gamma
# from jax.scipy.special import gammaln

from .multiscale_optimization import decrease_lengths_res, decrease_counts_res
from .multiscale_optimization import _count_fullres_per_lowres_bead
from .multiscale_optimization import _group_counts_multiscale
from .utils_poisson import find_beads_to_remove, _euclidean_distance
from .utils_poisson import _inter_counts, _counts_near_diag, _intra_mask
from .counts import ambiguate_counts, _ambiguate_beta
from .likelihoods import poisson_nll, gamma_poisson_nll, negbinom_nll
from .poisson import relu_min, get_gamma_params, get_gamma_moments


class Constraint(object):
    """Compute loss for the given constraint.

    Prepares cand computes the loss function for the given constraint.

    Parameters
    ----------
    lambda_val : float
        Lambda that specifies how strongly constraint is applied during
        calculation of the entire objective.
    lengths : array_like of int
        Number of beads per homolog of each chromosome.
    ploidy : {1, 2}
        Ploidy, 1 indicates haploid, 2 indicates diploid.
    multiscale_factor : int, optional
        Factor by which to reduce the resolution. A value of 2 halves the
        resolution. A value of 1 indicates full resolution.
    hparams : dict, optional
        Any hyperparameters used for the calculation of constraint.

    Attributes
    ----------
    abbrev : str
        Three-letter abbreviation for constraint name.
    name : str
        Full name of constraint
    during_alpha_infer : bool
        Whether or not this constraint should be computed during inference of
        alpha.
    lambda_val : float
        Lambda that specifies how strongly constraint is applied during
        calculation of the entire objective.
    lengths : array_like of int
        Number of beads per homolog of each chromosome.
    ploidy : {1, 2}
        Ploidy, 1 indicates haploid, 2 indicates diploid.
    multiscale_factor : int, optional
        Factor by which to reduce the resolution. A value of 2 halves the
        resolution. A value of 1 indicates full resolution.
    hparams : dict, optional
        Any hyperparameters used for the calculation of constraint.
    fullres_struct_nan : array of int
        Beads that should be removed (set to NaN) in the full-res structure.
    lowmem : bool, optional
        Whether variables generated by _setup should be saved for subsequent
        iterations.
    """

    def __init__(self, lambda_val, lengths, ploidy, multiscale_factor=1,
                 hparams=None, fullres_struct_nan=None, lowmem=False, mods=[]):
        self.abbrev = None
        self.name = None
        self.during_alpha_infer = None
        self.lambda_val = lambda_val
        self.lengths = lengths
        self.ploidy = ploidy
        self.multiscale_factor = multiscale_factor
        self.hparams = hparams
        self._fullres_struct_nan = fullres_struct_nan  # For self._setup() only
        self._lowmem = lowmem
        self._var = None

    def __str__(self):
        out = [f"CONSTRAINT: {self.name},  LAMBDA={self.lambda_val:g}"]
        if self.hparams is None:
            return "\n".join(out)
        for name, val in self.hparams.items():
            label = f"\t\t\t{name} = "
            if isinstance(val, (np.ndarray, list)):
                out.append(label + np.array2string(
                    val, formatter={'float_kind': lambda x: "%.3g" % x},
                    prefix=" " * len(label), separator=", "))
            elif isinstance(val, float):
                out.append(f"{label}{val:g}")
            else:
                out.append(f"{label}{val}")
        return "\n".join(out)

    def check(self):
        """Check constraints object.

        Check that lambdas are greater than zero, and that necessary
        hyperparameters are supplied."""
        pass

    def _setup(self, counts=None, bias=None):
        """Set up for applying constraint (not specific to inferred params)"""
        pass

    def apply(self, struct, alpha=None, epsilon=None, counts=None, bias=None,
              inferring_alpha=False):

        """Apply constraint using given structure(s).

        Compute loss for the given constraint.

        Parameters
        ----------
        struct : ndarray
            3D chromatin structure(s) for which to compute the constraint.
        alpha : float
            Biophysical parameter of the transfer function used in converting
            counts to wish distances. If alpha is not specified, it will be
            inferred.
        epsilon : float, optional
            TODO
        counts : list of CountsMatrix subclass instances
            Preprocessed counts data.
        bias : array of float, optional
            Biases computed by ICE normalization.
        inferring_alpha : bool, optional
            A value of "True" indicates that the current optimization aims to
            infer alpha, rather than the structure.

        Returns
        -------
        constraint_obj
            Loss for constraint.
        """
        pass


class BeadChainConnectivity2019(Constraint):
    """TODO"""

    def __init__(self, lambda_val, lengths, ploidy, multiscale_factor=1,
                 hparams=None, fullres_struct_nan=None, lowmem=False, mods=[]):
        self.abbrev = "bcc"
        self.name = "Bead-chain connectivity (2019)"
        self.during_alpha_infer = False
        self.lambda_val = lambda_val
        self.lengths = np.asarray(lengths)
        self.lengths_lowres = decrease_lengths_res(
            self.lengths, multiscale_factor=multiscale_factor)
        self.ploidy = ploidy
        self.multiscale_factor = multiscale_factor
        self.hparams = hparams
        self._fullres_struct_nan = None  # Not necessary for this constraint
        self._lowmem = lowmem
        self._var = None
        self.mods = mods  # TODO remove

        self.check()

    def check(self):
        if self.lambda_val < 0:
            raise ValueError("Constraint lambda may not be < 0.")
        if self.hparams is not None and len(self.hparams) > 0:
            raise ValueError(f"{self.name} constraint may not have hyperparams")

    def _setup(self, counts=None, bias=None):
        if self.lambda_val <= 0:
            return
        if self._var is not None:
            return self._var
        row_nghbr = _neighboring_bead_indices(
            lengths=self.lengths, ploidy=self.ploidy,
            multiscale_factor=self.multiscale_factor)
        var = {'row_nghbr': row_nghbr}
        if not self._lowmem:
            self._var = var
        return var

    def apply(self, struct, alpha=None, epsilon=None, counts=None, bias=None,
              inferring_alpha=False):
        if self.lambda_val == 0 or (
                inferring_alpha and not self.during_alpha_infer):
            return 0.
        var = self._setup(counts=counts, bias=bias)

        nghbr_dis = _euclidean_distance(
            struct, row=var['row_nghbr'], col=var['row_nghbr'] + 1)
        n_edges = nghbr_dis.shape[0]
        nghbr_dis_var = n_edges * ag_np.square(
            nghbr_dis).sum() / ag_np.square(nghbr_dis.sum())
        obj = nghbr_dis_var - 1.

        if ag_np.isnan(obj) or ag_np.isinf(obj):

            # if type(obj).__name__ in ('DeviceArray', 'ndarray'):
            print(f"{struct.mean()=:.3g}")  # TODO remove
            print(f"{np.isnan(struct).sum() / struct.size:.3g}")
            print(f"{nghbr_dis.mean()=:.3g}")
            print(f"{nghbr_dis.min()=:.3g}")
            print(f"{nghbr_dis_var.mean()=:.3g}")
            exit(1)

            raise ValueError(f"{self.name} constraint objective is {obj}.")
        return self.lambda_val * obj


class HomologSeparating2019(Constraint):
    """TODO"""

    def __init__(self, lambda_val, lengths, ploidy, multiscale_factor=1,
                 hparams=None, fullres_struct_nan=None, lowmem=False, mods=[]):
        self.abbrev = "hsc"
        self.name = "Homolog separating (2019)"
        self.during_alpha_infer = False
        self.lambda_val = lambda_val
        self.lengths = np.asarray(lengths)
        self.lengths_lowres = decrease_lengths_res(
            self.lengths, multiscale_factor=multiscale_factor)
        self.ploidy = ploidy
        self.multiscale_factor = multiscale_factor
        self.hparams = hparams
        self._fullres_struct_nan = fullres_struct_nan  # For self._setup() only
        self._lowmem = lowmem
        self._var = None
        self.mods = mods  # TODO remove

        self.check()

    def check(self):
        if self.ploidy == 1 and self.lambda_val > 0:
            raise ValueError(f"{self.name} constraint can not be applied to"
                             " haploid genomes.")
        if self.lambda_val < 0:
            raise ValueError("Constraint lambda may not be < 0.")
        # Hyperparam: perc_diff
        if self.hparams is None:
            self.hparams = {'perc_diff': None}
        if 'perc_diff' not in self.hparams:
            self.hparams['perc_diff'] = None
        if self.hparams['perc_diff'] is not None and (
                self.hparams['perc_diff'] < 0 or self.hparams['perc_diff'] > 1):
            raise ValueError("'perc_diff' must be between 0 and 1.")
        # Hyperparam: est_hmlg_sep
        if 'est_hmlg_sep' not in self.hparams or self.hparams[
                'est_hmlg_sep'] is None:
            raise ValueError(f"{self.name} constraint is missing neccessary"
                             " hyperparams: 'est_hmlg_sep'")
        if isinstance(self.hparams['est_hmlg_sep'], list):
            self.hparams['est_hmlg_sep'] = np.array(
                self.hparams['est_hmlg_sep'])
        if not isinstance(
                self.hparams['est_hmlg_sep'], (np.ndarray, float, int)):
            raise ValueError(f"{self.name} constraint hyperparam 'est_hmlg_sep'"
                             " not understood.")
        if isinstance(self.hparams['est_hmlg_sep'], np.ndarray):
            if self.hparams['est_hmlg_sep'].size not in (1, self.lengths.size):
                raise ValueError(f"{self.name} constraint hyperparam"
                                 " 'est_hmlg_sep' is not the correct size.")

    def _setup(self, counts=None, bias=None):
        if self.lambda_val <= 0:
            return
        if self._var is not None:
            return self._var

        if self.multiscale_factor > 1:
            fullres_per_lowres_bead = _count_fullres_per_lowres_bead(
                multiscale_factor=self.multiscale_factor, lengths=self.lengths,
                ploidy=self.ploidy, fullres_struct_nan=self._fullres_struct_nan)
            bead_weights = fullres_per_lowres_bead / self.multiscale_factor  # FIXME is this correct????
        else:
            bead_weights = np.ones((self.lengths_lowres.sum() * self.ploidy,))
        struct_nan = find_beads_to_remove(
            counts, lengths=self.lengths, ploidy=self.ploidy,
            multiscale_factor=self.multiscale_factor)
        if struct_nan.sum() != 0:
            raise ValueError("Check that we actually want to remove torm beads here...")
        bead_weights[struct_nan] = 0.
        n = self.lengths_lowres.sum()
        begin = end = 0
        for i in range(len(self.lengths_lowres)):
            end = end + self.lengths_lowres[i]
            bead_weights[:n][begin:end] /= np.sum(
                bead_weights[:n][begin:end])
            bead_weights[n:][begin:end] /= np.sum(
                bead_weights[n:][begin:end])
            begin = end
        bead_weights = bead_weights.reshape(-1, 1)

        var = {'bead_weights': bead_weights}
        if not self._lowmem:
            self._var = var
            self._fullres_struct_nan = None  # No longer needed unless lowmem
        return var

    def apply(self, struct, alpha=None, epsilon=None, counts=None, bias=None,
              inferring_alpha=False):
        if self.lambda_val == 0 or (
                inferring_alpha and not self.during_alpha_infer):
            return 0.
        var = self._setup(counts=counts, bias=bias)

        # Get homolog separation
        struct_bw = struct * np.repeat(var['bead_weights'], 3, axis=1)
        n = self.lengths_lowres.sum()
        hmlg_sep = ag_np.zeros(self.lengths_lowres.shape)
        begin = end = 0
        for i in range(self.lengths_lowres.size):
            end = end + ag_np.int32(self.lengths_lowres[i])
            chrom1_mean = ag_np.sum(struct_bw[begin:end], axis=0)
            chrom2_mean = ag_np.sum(struct_bw[(n + begin):(n + end)], axis=0)
            hmlg_sep_i = ag_np.sqrt(ag_np.sum(ag_np.square(
                chrom1_mean - chrom2_mean)))
            hmlg_sep = hmlg_sep.at[i].set(hmlg_sep_i)
            begin = end

        hmlg_sep_diff = self.hparams["est_hmlg_sep"] - hmlg_sep
        if 'rscale' in self.mods:
            hmlg_sep_diff = 1 - hmlg_sep / self.hparams["est_hmlg_sep"]
        if self.hparams['perc_diff'] is None:
            hmlg_sep_diff = relu(hmlg_sep_diff)
            raise ValueError("I thought we weren't doing RELU for HSC anymore")
        else:
            hsc_cutoff = ag_np.array(self.hparams['perc_diff'] * self.hparams[
                "est_hmlg_sep"])
            gt0 = hmlg_sep_diff > 0
            hmlg_sep_diff = hmlg_sep_diff.at[gt0].set(relu(
                hmlg_sep_diff[gt0] - hsc_cutoff[gt0]))
            hmlg_sep_diff = hmlg_sep_diff.at[~gt0].set(-relu(
                -(hmlg_sep_diff[~gt0] + hsc_cutoff[~gt0])))
        hmlg_sep_diff_sq = ag_np.square(hmlg_sep_diff)
        obj = ag_np.mean(hmlg_sep_diff_sq)

        if ag_np.isnan(obj) or ag_np.isinf(obj):
            raise ValueError(f"{self.name} constraint objective is {obj}.")
        return self.lambda_val * obj


class BeadChainConnectivity2021(Constraint):
    """TODO"""

    def __init__(self, lambda_val, lengths, ploidy, multiscale_factor=1,
                 hparams=None, fullres_struct_nan=None, lowmem=False, mods=[]):
        self.abbrev = "bcc"
        self.name = "Bead-chain connectivity (2021)"
        self.during_alpha_infer = True
        self.lambda_val = lambda_val
        self.lengths = np.asarray(lengths)
        self.lengths_lowres = decrease_lengths_res(
            self.lengths, multiscale_factor=multiscale_factor)
        self.ploidy = ploidy
        self.multiscale_factor = multiscale_factor
        self.hparams = hparams
        self._fullres_struct_nan = fullres_struct_nan  # For self._setup() only
        self._lowmem = lowmem
        self._var = None
        self.mods = mods  # TODO remove

        self.check()

    def check(self):
        if self.lambda_val < 0:
            raise ValueError("Constraint lambda may not be < 0.")
        if self.hparams is not None and len(self.hparams) > 0:
            raise ValueError(f"{self.name} constraint may not have hyperparams")

    def _setup(self, counts=None, bias=None):
        if self.lambda_val <= 0:
            return
        if self._var is not None:
            return self._var
        row_nghbr = _neighboring_bead_indices(
            lengths=self.lengths, ploidy=self.ploidy,
            multiscale_factor=self.multiscale_factor)

        beta = _ambiguate_beta(
            [c.beta for c in counts if c.sum() != 0], counts=counts,
            lengths=self.lengths, ploidy=self.ploidy)
        counts_ambig = ambiguate_counts(
            counts=counts, lengths=self.lengths, ploidy=self.ploidy,
            exclude_zeros=True)
        row_nghbr_ambig = _neighboring_bead_indices(
            lengths=self.lengths, ploidy=1,
            multiscale_factor=self.multiscale_factor, counts=counts_ambig,
            include_struct_nan_beads=False)

        if bias is not None and not np.all(bias == 1):
            raise NotImplementedError("Implement for bias... especially for multires")  # TODO

        counts_ambig = decrease_counts_res(
            counts_ambig, multiscale_factor=self.multiscale_factor,
            lengths=self.lengths, ploidy=self.ploidy)
        nghbr_counts = counts_ambig.diagonal(k=1)[row_nghbr_ambig] / self.ploidy
        if self.multiscale_factor > 1:
            fullres_per_lowres_bead = _count_fullres_per_lowres_bead(
                multiscale_factor=self.multiscale_factor, lengths=self.lengths,
                ploidy=1, fullres_struct_nan=self._fullres_struct_nan)
            raise NotImplementedError("Implement for multiscale")  # TODO

        var = {
            'row_nghbr': row_nghbr, 'beta': beta, 'nghbr_counts': nghbr_counts}
        if not self._lowmem:
            self._var = var
            self._fullres_struct_nan = None  # No longer needed unless lowmem
        return var

    def apply(self, struct, alpha=None, epsilon=None, counts=None, bias=None,
              inferring_alpha=False):
        if self.lambda_val == 0 or (
                inferring_alpha and not self.during_alpha_infer):
            return 0.
        elif alpha is None:
            raise ValueError(f"Must input alpha for {self.name} constraint.")
        var = self._setup(counts=counts, bias=bias)

        if (var['nghbr_counts'] == 0).sum() == 0:
            nghbr_dis = np.power(var['nghbr_counts'] / var['beta'], 1 / alpha)
            mu = nghbr_dis.mean()
            sigma_max = nghbr_dis.std()
        else:
            mu, sigma_max = taylor_approx_ndc(
                var['nghbr_counts'], beta=var['beta'], alpha=alpha, order=1)

        nghbr_dis = _euclidean_distance(
            struct, row=var['row_nghbr'], col=var['row_nghbr'] + 1)

        mad = ag_np.median(ag_np.absolute(
            nghbr_dis - ag_np.median(nghbr_dis)))
        sigma_tmp = 1.4826 * mad
        # sigma_tmp = ag_np.sqrt(ag_np.mean(ag_np.square(data - mu))) # old - i guess this didn't work

        if 'norm_dis_nomin' in self.mods:
            sigma = sigma_tmp
        else:
            sigma = relu_min(sigma_tmp, sigma_max)

        obj = ag_np.log(sigma) + ag_np.mean(
            ag_np.square(nghbr_dis - mu)) / (2 * ag_np.square(sigma))
        # if type(obj).__name__ == 'DeviceArray':
        #     data = nghbr_dis
        #     # TRUE:    μ=1.010   σMax=0.119      mean=1.004      sigma=0.098     sigma_tmp=0.098     std=0.098   obj=-1.8202
        #     # oldTRUE: μ=0.985     mean=1.004      sigma=0.098     std=0.098   obj=-1.8019
        #     # BCC1e1:         rmsd_intra=3.72   disterr_intra=39.9   disterr_interhmlg=70.8   ndv_nrmse=3.53  ()
        #     # norm_dis:       rmsd_intra=   disterr_intra=   disterr_interhmlg=   ndv_nrmse= ()
        #     # norm_dis_nomin: rmsd_intra=   disterr_intra=   disterr_interhmlg=   ndv_nrmse= ()
        #     print(f'μ={mu:.3f} \t σMax={sigma_max:.3f} \t mean={data.mean():.3f} \t sigma={sigma:.3f} \t sigma_tmp={sigma_tmp:.3f} \t std={data.std():.3f} \t obj={obj:.5g}')

        if ag_np.isnan(obj) or ag_np.isinf(obj):
            raise ValueError(f"{self.name} constraint objective is {obj}.")
        return self.lambda_val * obj


class BeadChainConnectivity2022(Constraint):
    """TODO"""

    def __init__(self, lambda_val, lengths, ploidy, multiscale_factor=1,
                 hparams=None, fullres_struct_nan=None, lowmem=False, mods=[]):
        self.abbrev = "bcc"
        self.name = "Bead-chain connectivity (2022)"
        self.during_alpha_infer = True
        self.lambda_val = lambda_val
        self.lengths = np.asarray(lengths)
        self.lengths_lowres = decrease_lengths_res(
            self.lengths, multiscale_factor=multiscale_factor)
        self.ploidy = ploidy
        self.multiscale_factor = multiscale_factor
        self.hparams = hparams
        self._fullres_struct_nan = None  # Not necessary for this constraint
        self._lowmem = lowmem
        self._var = None
        self.mods = mods  # TODO remove

        self.check()

    def check(self):
        if self.ploidy == 1 and self.lambda_val > 0:
            raise ValueError(f"{self.name} constraint can not be applied to"
                             " haploid genomes.")
        if self.lambda_val < 0:
            raise ValueError("Constraint lambda may not be < 0.")
        if self.hparams is None or 'counts_interchrom' not in self.hparams or (
                self.hparams['counts_interchrom'] is None):
            raise ValueError(f"{self.name} constraint is missing"
                             f" neccessary hyperparams: 'counts_interchrom'")

    def _setup(self, counts=None, bias=None):
        if self.lambda_val <= 0:
            return
        if self._var is not None:
            return self._var

        beta = _ambiguate_beta(
            [c.beta for c in counts if c.sum() != 0], counts=counts,
            lengths=self.lengths, ploidy=self.ploidy)

        # Get indices of neighboring beads
        row_nghbr = _neighboring_bead_indices(
            lengths=self.lengths, ploidy=self.ploidy,
            multiscale_factor=self.multiscale_factor)

        # Get bias corresponding to neighboring beads
        row_nghbr_ambig = _neighboring_bead_indices(
            lengths=self.lengths, ploidy=1,
            multiscale_factor=self.multiscale_factor)  # this is lowres rows
        if bias is None or np.all(bias == 1):
            bias_nghbr = 1
        else:
            raise NotImplementedError("Implement for multires!")
            # for this constraint, can probably actually just use fullres bias
            # & input into the gamma-poisson objective...
            bias_nghbr = bias.ravel()[row_nghbr_ambig] * bias.ravel()[
                row_nghbr_ambig + 1]

        # Get counts corresponding to neighboring beads
        counts = [c for c in counts if c.sum() != 0]
        counts_ambig = ambiguate_counts(
            counts=counts, lengths=self.lengths, ploidy=self.ploidy,
            exclude_zeros=False)
        if self.multiscale_factor == 1:
            counts_nghbr = counts_ambig[row_nghbr_ambig, row_nghbr_ambig + 1]
            counts_nghbr[np.isnan(counts_nghbr)] = np.nanmean(counts_nghbr)
        else:
            raise NotImplementedError

            counts_nghbr = _counts_near_diag(
                counts_ambig, self.lengths, ploidy=self.ploidy, nbins=1,
                exclude_zeros=False)

            tmp = _group_counts_multiscale(
                counts_nghbr, lengths=self.lengths, ploidy=self.ploidy,
                multiscale_factor=self.multiscale_factor)
            counts_nghbr, indices, indices3d, counts_shape, counts_mask = tmp

        var = {
            'row_nghbr': row_nghbr, 'counts_nghbr': counts_nghbr,
            'bias_nghbr': bias_nghbr, 'beta': beta}
        if not self._lowmem:
            self._var = var
        return var

    def apply(self, struct, alpha=None, epsilon=None, counts=None, bias=None,
              inferring_alpha=False):
        if self.lambda_val == 0 or (
                inferring_alpha and not self.during_alpha_infer):
            return 0.
        elif alpha is None:
            raise ValueError(f"Must input alpha for {self.name} constraint.")
        var = self._setup(counts=counts, bias=bias)

        nghbr_dis = _euclidean_distance(
            struct, row=var['row_nghbr'], col=var['row_nghbr'] + 1)
        lambda_nghbr = var['beta'] * var['bias_nghbr'] * ag_np.power(
            nghbr_dis, alpha)

        # TODO technically not poisson - continuous.... gamma?
        data = np.maximum(
            0, var['counts_nghbr'] - np.mean(
                self.hparams['counts_interchrom']) / 2)
        obj = poisson_nll(np.tile(data, 2), lambda_pois=2 * lambda_nghbr)

        if ag_np.isnan(obj) or ag_np.isinf(obj):
            raise ValueError(f"{self.name} constraint objective is {obj}.")
        return self.lambda_val * obj


class HomologSeparating2022(Constraint):
    """TODO"""

    def __init__(self, lambda_val, lengths, ploidy, multiscale_factor=1,
                 hparams=None, fullres_struct_nan=None, lowmem=False, mods=[]):
        self.abbrev = "hsc"
        self.name = "Homolog separating (2022)"
        self.during_alpha_infer = True
        self.lambda_val = lambda_val
        self.lengths = np.asarray(lengths)
        self.lengths_lowres = decrease_lengths_res(
            self.lengths, multiscale_factor=multiscale_factor)
        self.ploidy = ploidy
        self.multiscale_factor = multiscale_factor
        self.hparams = hparams
        self._fullres_struct_nan = fullres_struct_nan  # For self._setup() only
        self._lowmem = lowmem
        self._var = None
        self.mods = mods  # TODO remove

        self.check()

    def check(self):
        if self.ploidy == 1 and self.lambda_val > 0:
            raise ValueError(f"{self.name} constraint can not be applied to"
                             " haploid genomes.")
        if self.lambda_val < 0:
            raise ValueError("Constraint lambda may not be < 0.")
        # Hyperparam: perc_diff  # TODO maybe remove
        if self.hparams is None:
            self.hparams = {'perc_diff': None}
        if 'perc_diff' not in self.hparams:
            self.hparams['perc_diff'] = None
        if self.hparams['perc_diff'] is not None and (
                self.hparams['perc_diff'] < 0 or self.hparams['perc_diff'] > 1):
            raise ValueError("'perc_diff' must be between 0 and 1.")
        # Hyperparam: counts_interchrom
        if self.hparams is None or 'counts_interchrom' not in self.hparams or (
                self.hparams['counts_interchrom'] is None):
            raise ValueError(f"{self.name} constraint is missing"
                             f" neccessary hyperparams: 'counts_interchrom'")

    def _setup(self, counts=None, bias=None):
        if self.lambda_val <= 0:
            return
        if self._var is not None:
            return self._var

        beta = _ambiguate_beta(
            [c.beta for c in counts if c.sum() != 0], counts=counts,
            lengths=self.lengths, ploidy=self.ploidy)

        if self.multiscale_factor > 1:
            fullres_per_lowres_bead = _count_fullres_per_lowres_bead(
                multiscale_factor=self.multiscale_factor, lengths=self.lengths,
                ploidy=self.ploidy, fullres_struct_nan=self._fullres_struct_nan)
        else:
            fullres_per_lowres_bead = None

        if self.lengths_lowres.size > 1 and 'interhmlg_intrachrom' in self.mods:
            n = self.lengths_lowres.sum()
            # row, col = (x.flatten() for x in np.indices((n, n)))
            mask_intrachrom = _intra_mask(
                (n, n), lengths_at_res=self.lengths_lowres)
            # mask_intrachrom = (col > row) & mask_intrachrom
        else:
            mask_intrachrom = None

        var = {'beta': beta, 'fullres_per_lowres_bead': fullres_per_lowres_bead,
               'mask_intrachrom': mask_intrachrom}
        if not self._lowmem:
            self._var = var
            self._fullres_struct_nan = None  # No longer needed unless lowmem
        return var

    def apply(self, struct, alpha=None, epsilon=None, counts=None, bias=None,
              inferring_alpha=False):
        if self.lambda_val == 0 or (
                inferring_alpha and not self.during_alpha_infer):
            return 0.
        elif alpha is None:
            raise ValueError(f"Must input alpha for {self.name} constraint.")
        var = self._setup(counts=counts, bias=bias)

        # Get inter-homolog distances
        # n = int(struct.shape[0] / 2)
        n = self.lengths_lowres.sum()
        row, col = (x.flatten() for x in np.indices((n, n)))
        if 'interhmlg_intrachrom' in self.mods:
            row = row[var['mask_intrachrom']]
            col = col[var['mask_intrachrom']]
        dis_interhmlg = _euclidean_distance(struct, row=row, col=col + n)

        # Get inter-homolog bias
        if bias is None or np.all(bias == 1):
            bias_interhmlg = 1
        else:
            raise NotImplementedError("Implement for multires!")
            # I know we're not supposed to multiply counts by anything because
            # it messes with mean/var relationship, but if mean(bias) == 1,
            # can we do it anyways?
            # or maybe don't even need to worry about bias because it averages
            # out for the inter-chrom counts....
            bias_interhmlg = bias.ravel()[row] * bias.ravel()[col]

        counts_interchrom = self.hparams['counts_interchrom']

        if 'mse' in self.mods:
            dis_alpha_interhmlg = ag_np.power(dis_interhmlg, alpha)
            lambda_interhmlg = gamma_mean = (4 * var['beta']) * (
                bias_interhmlg * dis_alpha_interhmlg)
            if 'mean' in self.mods:
                lambda_interhmlg = lambda_interhmlg.reshape(n, n)
                nchrom = self.lengths.size
                interhmlg_mean = ag_np.zeros((nchrom, nchrom))
                begin_i = end_i = 0
                for i in range(nchrom):
                    end_i = end_i + ag_np.int32(self.lengths_lowres[i])
                    begin_j = end_j = 0
                    for j in range(nchrom):
                        end_j = end_j + ag_np.int32(self.lengths_lowres[j])
                        # print(f"i={i}=[{begin_i}:{end_i}],  j={j}=[{begin_j}:{end_j}]")
                        interhmlg_mean = interhmlg_mean.at[i, j].set(ag_np.mean(
                            lambda_interhmlg[begin_i:end_i, begin_j:end_j]))
                        begin_j = end_j
                    begin_i = end_i
                lambda_interhmlg = interhmlg_mean
                # print(lambda_interhmlg); exit(0)
            if 'flex' in self.mods:
                obj = _mse_flexible(
                    actual=lambda_interhmlg, expected=ag_np.mean(counts_interchrom),
                    cutoff=self.hparams['perc_diff'], scale_by_expected=True)
                # cutoff0 = _mse_flexible(
                #     actual=lambda_interhmlg, expected=ag_np.mean(counts_interchrom),
                #     cutoff=0, scale_by_expected=True)
                # if cutoff0 < obj:
                #     if type(obj).__name__ in ('DeviceArray', 'ndarray'):
                #         print(f"\n\nWHY??\nobj={obj._value}\ncutoff0={cutoff0._value}"); exit(1)
                #     else:
                #         print(obj)
                #         print(cutoff0)
                #         exit(1)
            else:
                obj = _mse_outside_of_window(
                    actual=lambda_interhmlg, expected=ag_np.mean(counts_interchrom),
                    cutoff=self.hparams['perc_diff'], scale_by_expected=True)
            if 'debug' in self.mods and type(obj).__name__ in ('DeviceArray', 'ndarray'):
                tmp = f'\t  σ²={lambda_interhmlg.var():.2g}'
                if 'mean' in self.mods:
                    tmp = tmp + '\t  λ=' + ', '.join([f"{x:.2g}" for x in lambda_interhmlg._value.ravel().tolist()])
                if epsilon is None:
                    print(f"c={np.mean(counts_interchrom):.3g}\tµ={lambda_interhmlg.mean():.3g}\t  obj={obj:.2g}{tmp}", flush=True)
                else:
                    print(f"c={np.mean(counts_interchrom):.3g}\tµ={lambda_interhmlg.mean():.3g}\t  obj={obj:.2g}\t  ε={epsilon:.2g}{tmp}", flush=True)
        elif 'gamma' in self.mods:
            if self.multiscale_factor > 1:
                # gamma_mean, gamma_var = get_gamma_moments(
                #     dis=dis_interhmlg, epsilon=epsilon, alpha=alpha,
                #     beta=4 * var['beta'], ambiguity='ua')
                theta, k = get_gamma_params(
                    dis=dis_interhmlg, epsilon=epsilon, alpha=alpha,
                    beta=4 * var['beta'], ambiguity='ua')
            else:
                dis_alpha_interhmlg = ag_np.power(dis_interhmlg, alpha)
                lambda_interhmlg = gamma_mean = (4 * var['beta']) * (
                    bias_interhmlg * dis_alpha_interhmlg)
                theta = 1e6
                k = gamma_mean / theta
            obj = -logpdf_gamma(np.mean(counts_interchrom), a=k, scale=theta).mean()
            if 'debug' in self.mods and type(obj).__name__ in ('DeviceArray', 'ndarray'):
                if epsilon is None:
                    print(f"c={np.mean(counts_interchrom):.3g}\t   mean={lambda_interhmlg.mean():.3g}\t   obj={obj:.3g}", flush=True)
                else:
                    dis_alpha_interhmlg = ag_np.power(dis_interhmlg, alpha)
                    lambda_interhmlg = (4 * var['beta']) * (
                        bias_interhmlg * dis_alpha_interhmlg)
                    print(f"c={np.mean(counts_interchrom):.3g}\t   λ={lambda_interhmlg.mean():.3g}\t   Γ={(k * theta).mean():.3g}\t   obj={obj:.3g}\t   ε={epsilon:.3g}", flush=True)

        elif self.multiscale_factor > 1 and 'nb_model' in self.mods:
            theta, k = get_gamma_params(
                dis=dis_interhmlg, epsilon=epsilon, alpha=alpha,
                beta=4 * var['beta'], ambiguity='ua')

            # fullres_per_lowres_bins = var['fullres_per_lowres_bead'][row] * (
            #     var['fullres_per_lowres_bead'][col + n])

            counts_interchrom = np.array([counts_interchrom.mean()])
            counts_interchrom = counts_interchrom.reshape(-1, 1)
            obj = gamma_poisson_nll(
                theta=theta, k=k, data=counts_interchrom, bias=bias,
                num_fullres_per_lowres_bins=None,
                mods=self.mods)
            if 'debug' in self.mods and type(obj).__name__ in ('DeviceArray', 'ndarray') and False:
                dis_alpha_interhmlg = ag_np.power(dis_interhmlg, alpha)
                lambda_interhmlg = (4 * var['beta']) * (
                    bias_interhmlg * dis_alpha_interhmlg)
                print(f"c={np.mean(counts_interchrom):.3g}\t   mean={lambda_interhmlg.mean():.3g}\t   obj={obj:.3g}\t   ε={epsilon:.3g}", flush=True)
        else:
            dis_alpha_interhmlg = ag_np.power(dis_interhmlg, alpha)
            lambda_interhmlg = (4 * var['beta']) * (
                bias_interhmlg * dis_alpha_interhmlg)

            if self.multiscale_factor > 1:
                fullres_per_lowres_bins = var['fullres_per_lowres_bead'][row] * (
                    var['fullres_per_lowres_bead'][col + n])
            else:
                fullres_per_lowres_bins = 1

            if 'nbinom' in self.mods:
                # logpmf_negbinom(k, n, p)
                # FIXME do something about fullres_per_lowres_bins
                nb_mean = lambda_interhmlg
                if self.multiscale_factor > 1:
                    nb_mean = nb_mean * np.square(self.multiscale_factor)

                counts_var = counts_interchrom.var()
                nb_var = relu_max(counts_var, nb_mean)

                mods_numeric = [x for x in self.mods if re.match(r'^[1-9][0-9]*$', x)]
                if len(mods_numeric) > 1:
                    raise ValueError("see this is what happens when you do dumb hacks")
                if len(mods_numeric) == 1:
                    lim = int(mods_numeric[0])
                    counts_interchrom = counts_interchrom[:lim]  # FIXME (out of 22500)

                nb_mask = nb_mean != nb_var
                # obj_pois = -logpmf_poisson(
                #     counts_interchrom.reshape(-1, 1),
                #     mu=nb_mean[~nb_mask])
                if (~nb_mask).sum() == 0:
                    obj_pois = 0
                else:
                    obj_pois = poisson_nll(
                        counts_interchrom.mean(), lambda_pois=nb_mean[~nb_mask],
                        mean=False)

                # log_factorial_counts = gammaln(counts_interchrom + 1).mean()
                # print(gammaln(counts_interchrom + 1).mean())

                # obj_test = poisson_nll(
                #     counts_interchrom.mean(), lambda_pois=nb_mean[~nb_mask]) + log_factorial_counts
                # print(ag_np.isclose(obj_pois.mean(), obj_test))

                p = nb_mean[nb_mask] / counts_var
                n = ag_np.square(nb_mean[nb_mask]) / (counts_var - nb_mean[nb_mask])
                # obj_nb = -logpmf_negbinom(
                #     counts_interchrom.reshape(-1, 1), n=n, p=p) - log_factorial_counts
                if nb_mask.sum() == 0:
                    obj_nb = 0
                else:
                    obj_nb = negbinom_nll(
                        counts_interchrom.reshape(-1, 1), n=n, p=p, mean=False)

                # obj_test = negbinom_nll(
                #     counts_interchrom.reshape(-1, 1), n=n, p=p)
                # if nb_mask.sum() > 0 and type(obj_nb).__name__ in ('DeviceArray', 'ndarray'):
                #     if not ag_np.isclose(obj_nb.mean(), obj_test):
                #         print(obj_nb.mean()); print(obj_test); exit(1)

                # obj = ((obj_nb * nb_mask.sum()) + (obj_pois * (~nb_mask).sum())) / row.size
                obj = (obj_nb + obj_pois) / row.size
                if 'debug' in self.mods and type(obj).__name__ in ('DeviceArray', 'ndarray'):
                    to_print = f"c={counts_interchrom.mean():.2g}\t   μ={nb_mean.mean():.2g}\t   σ²={nb_var.mean():.2g}\t   pois={obj_pois / (~nb_mask).sum():.2g}\t   nb={obj_nb / nb_mask.sum():.2g}\t   {nb_mask.sum() / nb_mask.size * 100:.2g}% nb"
                    if epsilon is not None:
                        to_print += f"\t   ε={epsilon:.2g}"
                    print(to_print, flush=True)
                # exit(1)
            else:
                counts_interchrom = np.mean(counts_interchrom)
                if 'no_nxny' not in self.mods:
                    counts_interchrom = counts_interchrom * fullres_per_lowres_bins
                    lambda_interhmlg = lambda_interhmlg * fullres_per_lowres_bins
                obj = poisson_nll(
                    counts_interchrom, lambda_pois=lambda_interhmlg)
                if 'debug' in self.mods and type(obj).__name__ in ('DeviceArray', 'ndarray'):
                    if epsilon is None:
                        print(f"c={np.mean(counts_interchrom):.3g}\t   mean={lambda_interhmlg.mean():.3g}\t   obj={obj:.3g}", flush=True)
                    else:
                        print(f"c={np.mean(counts_interchrom):.3g}\t   mean={lambda_interhmlg.mean():.3g}\t   obj={obj:.3g}\t   ε={epsilon:.3g}", flush=True)

        if ag_np.isnan(obj) or ag_np.isinf(obj):
            raise ValueError(f"{self.name} constraint objective is {obj}.")
        return self.lambda_val * obj


def _mse_flexible(actual, expected, cutoff=None, scale_by_expected=True):
    """TODO"""

    if scale_by_expected:
        actual = actual / expected
        expected = 1

    if cutoff is not None and cutoff != 0:
        if scale_by_expected:
            window = cutoff
        else:
            window = cutoff * expected

        mle_expected = ag_np.mean(actual)

        # print(f"{window=:g}")
        # print(f"{expected=:g}")
        # print(f"{mle_expected=:g}")

        sub_from_expected = relu_min(relu(expected - mle_expected), window)
        add_to_expected = relu_min(relu(mle_expected - expected), window)
        expected = expected + add_to_expected - sub_from_expected

        # print(f"{add_to_expected=:g}")
        # print(f"{sub_from_expected=:g}")
        # print(f"{expected=:g}")

    diff = expected - actual
    mse = ag_np.mean(ag_np.square(diff))

    return mse


def _mse_outside_of_window(actual, expected, cutoff=None, scale_by_expected=True, new=True):
    """TODO"""
    if scale_by_expected:
        diff = 1 - actual / expected
    else:
        diff = expected - actual

    if cutoff is None:
        diff = relu(diff)
        mse = ag_np.mean(ag_np.square(diff))
        raise ValueError("I thought we weren't doing ReLU for HSC anymore")  # TODO remove ValueError
        return mse

    if cutoff == 0:
        mse = ag_np.mean(ag_np.square(diff))
        return mse

    if scale_by_expected:
        window = cutoff
    else:
        window = cutoff * expected

    if new:
        n = diff.size
        diff_gt0 = relu(diff)
        diff_lt0 = -relu(-diff)
        diff_gt0 = relu(diff_gt0 - window)
        diff_lt0 = -relu(-(diff_lt0 + window))
        mse = (ag_np.square(diff_gt0).sum() + ag_np.square(diff_lt0).sum()) / n
    else:  # TODO remove old code
        window = ag_np.array(window)
        if window.size == 1 and actual.size > 1:
            window = ag_np.tile(window, actual.size)
        gt0 = diff > 0
        diff = diff.at[gt0].set(relu(diff[gt0] - window[gt0]))
        diff = diff.at[~gt0].set(-relu(-(diff[~gt0] + window[~gt0])))
        mse = ag_np.mean(ag_np.square(diff))

    return mse


def relu_max(x1, x2):  # TODO move to likelihoods.py
    # TODO this is temporary, remove this and switch to jax_max
    # returns max(x1, x2)
    return (relu((x1) - (x2)) + (x2))


def prep_constraints(counts, lengths, ploidy, multiscale_factor=1,
                     bcc_lambda=0., hsc_lambda=0., bcc_version='2019',
                     hsc_version='2019', counts_interchrom=None,
                     est_hmlg_sep=None, hsc_perc_diff=None,
                     fullres_struct_nan=None, verbose=True, mods=[]):
    """TODO"""

    # TODO remove
    if mods is None:
        mods = []
    elif isinstance(mods, str):
        mods = mods.lower().split('.')
    else:
        mods = [x.lower() for x in mods]

    if bcc_version is None:
        bcc_version = '2019'
    bcc_version = str(bcc_version)
    if hsc_version is None:
        hsc_version = '2019'
    hsc_version = str(hsc_version)

    if (bcc_lambda != 0 and bcc_version == '2022') or (
            hsc_lambda != 0 and hsc_version == '2022'):
        if counts_interchrom is None:
            counts_interchrom = get_counts_interchrom(
                counts, lengths=lengths, ploidy=ploidy,
                multiscale_factor=multiscale_factor,
                fullres_struct_nan=fullres_struct_nan, verbose=verbose, mods=mods)
            if not ('nbinom' in mods or 'nb_model' in mods):
                counts_interchrom = counts_interchrom.mean()
        elif isinstance(counts_interchrom, str):
            if multiscale_factor > 1:
                raise ValueError("Use actual interchrom counts")
            try:
                counts_interchrom = float(counts_interchrom)
            except ValueError:
                counts_interchrom = np.loadtxt(counts_interchrom)

        # if isinstance(counts_interchrom, np.ndarray):  # FIXME
        #     rng = np.random.default_rng(0)
        #     size = 50
        #     tmp = rng.choice(counts_interchrom, size=size, replace=False)
        #     i = 0
        #     print(counts_interchrom.mean(), counts_interchrom.var())
        #     while i < 200 and not (np.isclose(
        #             counts_interchrom.mean(), tmp.mean(), rtol=1e-01) and np.isclose(
        #             counts_interchrom.var(), tmp.var(), rtol=1e-01)):
        #         tmp = rng.choice(counts_interchrom, size=size, replace=False)
        #         i += 1
        #         print(i)
        #     print(tmp.mean(), tmp.var())
        #     if i == 200:
        #         print("ran out of i", flush=True)
        #         exit(1)
        #     counts_interchrom = tmp

    bcc_class = {
        '2019': BeadChainConnectivity2019, '2021': BeadChainConnectivity2021,
        '2022': BeadChainConnectivity2022}
    hsc_class = {
        '2019': HomologSeparating2019, '2022': HomologSeparating2022}
    bcc_hparams = {
        '2019': None, '2021': None,
        '2022': {'counts_interchrom': counts_interchrom}}
    hsc_hparams = {
        '2019': {'est_hmlg_sep': est_hmlg_sep, 'perc_diff': hsc_perc_diff},
        '2022': {'counts_interchrom': counts_interchrom,
                 'perc_diff': hsc_perc_diff}}

    constraints = []
    if bcc_lambda != 0:
        constraints.append(bcc_class[bcc_version](
            bcc_lambda, lengths=lengths, ploidy=ploidy,
            multiscale_factor=multiscale_factor,
            hparams=bcc_hparams[bcc_version],
            fullres_struct_nan=fullres_struct_nan, lowmem=False, mods=mods))
    if hsc_lambda != 0:
        constraints.append(hsc_class[hsc_version](
            hsc_lambda, lengths=lengths, ploidy=ploidy,
            multiscale_factor=multiscale_factor,
            hparams=hsc_hparams[hsc_version],
            fullres_struct_nan=fullres_struct_nan, lowmem=False, mods=mods))

    # TODO have var=None input to Constraint._setup = share var dict for all (only add to var if not already found)
    # TODO if NOT lowmem, run Constraint._setup method here = share var dict for all

    # TODO have var=None input to Constraint.apply = share var dict for all during opt if lowmem
    # TODO if lowmem, run Constraint._setup before .apply during opt = share var dict for all during opt if lowmem

    return constraints


def get_fullres_counts_interchrom(counts, lengths, ploidy, mods=[],
                                  verbose=False):
    """TODO"""

    if lengths.size == 1:
        raise ValueError(
            "Must input counts_interchrom if inferring single chromosome.")
    counts_ambig = ambiguate_counts(
        counts=counts, lengths=lengths, ploidy=ploidy, exclude_zeros=False)
    counts_interchrom = _inter_counts(
        counts_ambig, lengths, ploidy=ploidy, exclude_zeros=False)
    counts_interchrom = counts_interchrom[~np.isnan(counts_interchrom)]
    if verbose:
        print(f"Mean inter-chromosomal counts={counts_interchrom.mean():.3g}",
              flush=True)
    return counts_interchrom


def get_counts_interchrom(counts, lengths, ploidy, multiscale_factor=1,
                          fullres_struct_nan=None, verbose=False, mods=[]):
    """TODO"""

    if lengths.size == 1:
        raise ValueError(
            "Must input counts_interchrom if inferring single chromosome.")
    lengths_lowres = decrease_lengths_res(
        lengths, multiscale_factor=multiscale_factor)

    counts_ambig = ambiguate_counts(
        counts=counts, lengths=lengths_lowres, ploidy=ploidy,
        exclude_zeros=False)
    counts_interchrom = _inter_counts(
        counts_ambig, lengths_at_res=lengths_lowres, ploidy=ploidy,
        exclude_zeros=False)
    if multiscale_factor > 1:
        fullres_per_lowres_bead = _count_fullres_per_lowres_bead(
            multiscale_factor=multiscale_factor, lengths=lengths,
            ploidy=1, fullres_struct_nan=fullres_struct_nan)
        mask = fullres_per_lowres_bead == multiscale_factor
        counts_interchrom[~mask, :] = np.nan
        counts_interchrom[:, ~mask] = np.nan

    counts_interchrom = counts_interchrom[~np.isnan(counts_interchrom)]
    if verbose:
        print(f"Mean inter-chromosomal counts={counts_interchrom.mean():.3g}",
              flush=True)
    return counts_interchrom


def taylor_approx_ndc(x, beta=1, alpha=-3, order=1):
    x = x / beta
    x_mean = np.mean(x)
    x_var = np.var(x)

    fx_mean = np.power(x_mean, 1 / alpha)
    fx_var = 1 / np.power(alpha, 2) * np.power(
        x_mean, (2 - 2 * alpha) / alpha) * x_var

    if order == 2:  # FIXME TODO
        tmp = (1 - alpha) / (2 * np.square(alpha)) * np.power(
            x_mean, (1 - 2 * alpha) / alpha)
        fx_mean = fx_mean + tmp * x_var
        fx_var = fx_var + np.square(tmp) * (np.var(
            np.square(x)) - 4 * np.square(x_mean) * x_var)

    fx_std = np.sqrt(fx_var)
    return fx_mean, fx_std


def _neighboring_bead_indices(lengths, ploidy, multiscale_factor=1,
                              counts=None, include_struct_nan_beads=True):
    """Return row & col of neighboring beads, along a homolog of a chromosome.
    """

    lengths_lowres = decrease_lengths_res(lengths, multiscale_factor)
    nbeads = lengths_lowres.sum() * ploidy

    row_nghbr = np.arange(nbeads - 1, dtype=int)

    # Optionally remove beads for which there is no counts data
    if not include_struct_nan_beads:
        if counts is None:
            raise ValueError(
                "Counts must be inputted if including struct_nan beads.")
        struct_nan = find_beads_to_remove(
            counts, lengths=lengths, ploidy=ploidy,
            multiscale_factor=multiscale_factor)
        nghbr_dis_mask = (~np.isin(row_nghbr, struct_nan)) & (
            ~np.isin(row_nghbr + 1, struct_nan))
        row_nghbr = row_nghbr[nghbr_dis_mask]

    # Remove if "neighbor" beads are actually on different chromosomes
    # or homologs
    bins = np.tile(lengths_lowres, ploidy).cumsum()
    same_bin = np.digitize(row_nghbr, bins) == np.digitize(row_nghbr + 1, bins)

    row_nghbr = row_nghbr[same_bin]

    return row_nghbr


def _inter_homolog_dis(struct, lengths):
    """Computes distance between homologs for a normal diploid structure.
    """

    struct = struct.copy().reshape(-1, 3)

    n = int(struct.shape[0] / 2)
    homo1 = struct[:n, :]
    homo2 = struct[n:, :]

    hmlg_dis = []
    begin = end = 0
    for i in range(lengths.size):
        end += lengths[i]
        if np.isnan(homo1[begin:end, 0]).sum() == lengths[i] or np.isnan(
                homo2[begin:end, 0]).sum() == lengths[i]:
            hmlg_dis.append(np.nan)
        else:
            hmlg_dis.append(((np.nanmean(homo1[
                begin:end, :], axis=0) - np.nanmean(
                homo2[begin:end, :], axis=0)) ** 2).sum() ** 0.5)
        begin = end

    hmlg_dis = np.array(hmlg_dis)
    hmlg_dis[np.isnan(hmlg_dis)] = np.nanmean(hmlg_dis)

    return hmlg_dis


def _inter_homolog_dis_via_simple_diploid(struct, lengths):
    """Computes distance between chromosomes for a faux-haploid structure.
    """

    from sklearn.metrics import euclidean_distances

    struct = struct.copy().reshape(-1, 3)

    chrom_barycenters = []
    begin = end = 0
    for i in range(lengths.size):
        end += lengths[i]
        if np.isnan(struct[begin:end, 0]).sum() < lengths[i]:
            chrom_barycenters.append(
                np.nanmean(struct[begin:end, :], axis=0).reshape(1, 3))
        begin = end

    chrom_barycenters = np.concatenate(chrom_barycenters)

    hmlg_dis = euclidean_distances(chrom_barycenters)
    hmlg_dis[np.tril_indices(hmlg_dis.shape[0])] = np.nan

    return np.full(lengths.shape, np.nanmean(hmlg_dis))


def distance_between_homologs(structures, lengths, mixture_coefs=None,
                              simple_diploid=False):
    """Computes distances between homologs for a given structure.

    For diploid organisms, this computes the distance between homolog centers
    of mass for each chromosome.

    Parameters
    ----------
    structures : array of float or list of array of float
        3D chromatin structure(s) for which to assess inter-homolog distances.
    lengths : array_like of int
        Number of beads per homolog of each chromosome.
    simple_diploid: bool, optional
        For diploid organisms: whether the structure is an inferred "simple
        diploid" structure in which homologs are assumed to be identical and
        completely overlapping with one another.

    Returns
    -------
    array of float
        Distance between homologs per chromosome.

    """

    from .utils_poisson import _format_structures

    structures = _format_structures(
        structures=structures, lengths=lengths,
        ploidy=(1 if simple_diploid else 2),
        mixture_coefs=mixture_coefs)

    hmlg_dis = []
    for struct in structures:
        if simple_diploid:
            hmlg_dis.append(_inter_homolog_dis_via_simple_diploid(
                struct=struct, lengths=lengths))
        else:
            hmlg_dis.append(_inter_homolog_dis(struct=struct, lengths=lengths))

    return np.mean(hmlg_dis, axis=0)
