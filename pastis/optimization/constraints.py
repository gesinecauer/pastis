import sys

if sys.version_info[0] < 3:
    raise Exception("Must be using Python 3")

import numpy as np

from .utils_poisson import _setup_jax
_setup_jax()
import jax.numpy as ag_np
from jax.nn import relu
from jax.scipy.stats.nbinom import logpmf as logpmf_negbinom
from .multiscale_optimization import decrease_lengths_res
from .multiscale_optimization import _count_fullres_per_lowres_bead
from .utils_poisson import find_beads_to_remove, _euclidean_distance
from .utils_poisson import relu_min
from .utils_poisson import _inter_counts, _intra_mask
from .counts import ambiguate_counts, _ambiguate_beta, _get_nonzero_mask
from .counts import _best_counts_dtype
from .likelihoods import poisson_nll, gamma_poisson_nll
from .poisson import get_gamma_moments
from ..io.read import load_data


class Constraint(object):
    """Compute loss for the given constraint.

    Prepares cand computes the loss function for the given constraint.

    Parameters
    ----------
    lambda_val : float
        Lambda that specifies how strongly constraint is applied during
        calculation of the entire objective.
    lengths : array_like of int
        Number of beads per homolog of each chromosome.
    ploidy : {1, 2}
        Ploidy, 1 indicates haploid, 2 indicates diploid.
    multiscale_factor : int, optional
        Factor by which to reduce the resolution. A value of 2 halves the
        resolution. A value of 1 indicates full resolution.
    hparams : dict, optional
        Any hyperparameters used for the calculation of constraint.

    Attributes
    ----------
    abbrev : str
        Three-letter abbreviation for constraint name.
    name : str
        Full name of constraint
    during_alpha_infer : bool
        Whether or not this constraint should be computed during inference of
        alpha.
    lambda_val : float
        Lambda that specifies how strongly constraint is applied during
        calculation of the entire objective.
    lengths : array_like of int
        Number of beads per homolog of each chromosome.
    ploidy : {1, 2}
        Ploidy, 1 indicates haploid, 2 indicates diploid.
    multiscale_factor : int, optional
        Factor by which to reduce the resolution. A value of 2 halves the
        resolution. A value of 1 indicates full resolution.
    hparams : dict, optional
        Any hyperparameters used for the calculation of constraint.
    fullres_struct_nan : array of int
        Beads that should be removed (set to NaN) in the full-res structure.
    lowmem : bool, optional
        Whether variables generated by _setup should be saved for subsequent
        iterations.
    """

    def __init__(self, lambda_val, lengths, ploidy, multiscale_factor=1,
                 hparams=None, fullres_struct_nan=None, lowmem=False, mods=[]):
        self.abbrev = None
        self.name = None
        self.during_alpha_infer = None
        self.lambda_val = lambda_val
        self.lengths = lengths
        self.ploidy = ploidy
        self.multiscale_factor = multiscale_factor
        self.hparams = hparams
        self._fullres_struct_nan = fullres_struct_nan  # For self._setup() only
        self._lowmem = lowmem
        self._var = None

    def __str__(self):
        out = [f"CONSTRAINT: {self.name},  LAMBDA={self.lambda_val:g}"]
        if self.hparams is None:
            return "\n".join(out)
        for name, val in self.hparams.items():
            label = f"\t\t\t{name} = "
            if isinstance(val, (np.ndarray, list)):
                out.append(label + np.array2string(
                    val, formatter={'float_kind': lambda x: "%.3g" % x},
                    prefix=" " * len(label), separator=", "))
            elif isinstance(val, float):
                out.append(f"{label}{val:g}")
            else:
                out.append(f"{label}{val}")
        return "\n".join(out)

    def check(self):
        """Check constraints object.

        Check that lambdas are greater than zero, and that necessary
        hyperparameters are supplied."""
        pass

    def _setup(self, counts=None, bias=None):
        """Set up for applying constraint (not specific to inferred params)"""
        pass

    def apply(self, struct, alpha=None, epsilon=None, counts=None, bias=None,
              inferring_alpha=False):

        """Apply constraint using given structure(s).

        Compute loss for the given constraint.

        Parameters
        ----------
        struct : ndarray
            3D chromatin structure(s) for which to compute the constraint.
        alpha : float
            Biophysical parameter of the transfer function used in converting
            counts to wish distances. If alpha is not specified, it will be
            inferred.
        epsilon : float, optional
            TODO
        counts : list of CountsMatrix subclass instances
            Preprocessed counts data.
        bias : array of float, optional
            Biases computed by ICE normalization.
        inferring_alpha : bool, optional
            A value of "True" indicates that the current optimization aims to
            infer alpha, rather than the structure.

        Returns
        -------
        constraint_obj
            Loss for constraint.
        """
        pass


class BeadChainConnectivity2019(Constraint):
    """TODO"""

    def __init__(self, lambda_val, lengths, ploidy, multiscale_factor=1,
                 hparams=None, fullres_struct_nan=None, lowmem=False, mods=[]):
        self.abbrev = "bcc"
        self.name = "Bead-chain connectivity (2019)"
        self.during_alpha_infer = False
        self.lambda_val = lambda_val
        self.lengths = np.asarray(lengths)
        self.lengths_lowres = decrease_lengths_res(
            self.lengths, multiscale_factor=multiscale_factor)
        self.ploidy = ploidy
        self.multiscale_factor = multiscale_factor
        self.hparams = hparams
        self._fullres_struct_nan = None  # Not necessary for this constraint
        self._lowmem = lowmem
        self._var = None
        self.mods = mods  # TODO remove

        self.check()

    def check(self):
        if self.lambda_val < 0:
            raise ValueError("Constraint lambda may not be < 0.")
        if self.hparams is not None and len(self.hparams) > 0:
            raise ValueError(f"{self.name} constraint may not have hyperparams")

    def _setup(self, counts=None, bias=None):
        if self.lambda_val <= 0:
            return
        if self._var is not None:
            return self._var
        row_nghbr = _neighboring_bead_indices(
            lengths=self.lengths, ploidy=self.ploidy,
            multiscale_factor=self.multiscale_factor)
        var = {'row_nghbr': row_nghbr}
        if not self._lowmem:
            self._var = var
        return var

    def apply(self, struct, alpha=None, epsilon=None, counts=None, bias=None,
              inferring_alpha=False):
        if self.lambda_val == 0 or (
                inferring_alpha and not self.during_alpha_infer):
            return 0.
        var = self._setup(counts=counts, bias=bias)

        nghbr_dis = _euclidean_distance(
            struct, row=var['row_nghbr'], col=var['row_nghbr'] + 1)
        n_edges = nghbr_dis.shape[0]
        nghbr_dis_var = n_edges * ag_np.square(
            nghbr_dis).sum() / ag_np.square(nghbr_dis.sum())
        obj = nghbr_dis_var - 1.

        if not ag_np.isfinite(obj):

            # if type(obj).__name__ in ('DeviceArray', 'ndarray'):
            print(f"{struct.mean()=:.3g}")  # TODO remove
            print(f"{np.isnan(struct).sum() / struct.size:.3g}")
            print(f"{nghbr_dis.mean()=:.3g}")
            print(f"{nghbr_dis.min()=:.3g}")
            print(f"{nghbr_dis_var.mean()=:.3g}")
            exit(1)

            raise ValueError(f"{self.name} constraint is {obj}.")
        return self.lambda_val * obj


class HomologSeparating2019(Constraint):
    """TODO"""

    def __init__(self, lambda_val, lengths, ploidy, multiscale_factor=1,
                 hparams=None, fullres_struct_nan=None, lowmem=False, mods=[]):
        self.abbrev = "hsc"
        self.name = "Homolog separating (2019)"
        self.during_alpha_infer = False
        self.lambda_val = lambda_val
        self.lengths = np.asarray(lengths)
        self.lengths_lowres = decrease_lengths_res(
            self.lengths, multiscale_factor=multiscale_factor)
        self.ploidy = ploidy
        self.multiscale_factor = multiscale_factor
        self.hparams = hparams
        self._fullres_struct_nan = fullres_struct_nan  # For self._setup() only
        self._lowmem = lowmem
        self._var = None
        self.mods = mods  # TODO remove

        self.check()

    def check(self):
        if self.ploidy == 1 and self.lambda_val > 0:
            raise ValueError(f"{self.name} constraint can not be applied to"
                             " haploid genomes.")
        if self.lambda_val < 0:
            raise ValueError("Constraint lambda may not be < 0.")
        # Hyperparam: perc_diff
        if self.hparams is None:
            self.hparams = {'perc_diff': None}
        if 'perc_diff' not in self.hparams:
            self.hparams['perc_diff'] = None
        if self.hparams['perc_diff'] is not None and (
                self.hparams['perc_diff'] < 0 or self.hparams['perc_diff'] > 1):
            raise ValueError("'perc_diff' must be between 0 and 1.")
        # Hyperparam: est_hmlg_sep
        if 'est_hmlg_sep' not in self.hparams or self.hparams[
                'est_hmlg_sep'] is None:
            raise ValueError(f"{self.name} constraint is missing neccessary"
                             " hyperparams: 'est_hmlg_sep'")
        if isinstance(self.hparams['est_hmlg_sep'], list):
            self.hparams['est_hmlg_sep'] = np.array(
                self.hparams['est_hmlg_sep'])
        if not isinstance(
                self.hparams['est_hmlg_sep'], (np.ndarray, float, int)):
            raise ValueError(f"{self.name} constraint hyperparam 'est_hmlg_sep'"
                             " not understood.")
        if isinstance(self.hparams['est_hmlg_sep'], np.ndarray):
            if self.hparams['est_hmlg_sep'].size not in (1, self.lengths.size):
                raise ValueError(f"{self.name} constraint hyperparam"
                                 " 'est_hmlg_sep' is not the correct size.")

    def _setup(self, counts=None, bias=None):
        if self.lambda_val <= 0:
            return
        if self._var is not None:
            return self._var

        if self.multiscale_factor > 1:
            fullres_per_lowres_bead = _count_fullres_per_lowres_bead(
                multiscale_factor=self.multiscale_factor, lengths=self.lengths,
                ploidy=self.ploidy, fullres_struct_nan=self._fullres_struct_nan)
            bead_weights = fullres_per_lowres_bead / self.multiscale_factor  # FIXME is this correct????
        else:
            bead_weights = np.ones((self.lengths_lowres.sum() * self.ploidy,))
        struct_nan = find_beads_to_remove(
            counts, lengths=self.lengths, ploidy=self.ploidy,
            multiscale_factor=self.multiscale_factor)
        # if struct_nan.size != 0:
        #     raise ValueError("Check that we actually want to remove torm beads here...") # FIXME
        bead_weights[struct_nan] = 0
        n = self.lengths_lowres.sum()
        begin = end = 0
        for i in range(len(self.lengths_lowres)):
            end = end + self.lengths_lowres[i]
            bead_weights[begin:end] /= np.sum(bead_weights[begin:end])
            bead_weights[n + begin:n + end] /= np.sum(
                bead_weights[n + begin:n + end])
            begin = end
        bead_weights = bead_weights.reshape(-1, 1)

        var = {'bead_weights': bead_weights}
        if not self._lowmem:
            self._var = var
            self._fullres_struct_nan = None  # No longer needed unless lowmem
        return var

    def apply(self, struct, alpha=None, epsilon=None, counts=None, bias=None,
              inferring_alpha=False):
        if self.lambda_val == 0 or (
                inferring_alpha and not self.during_alpha_infer):
            return 0.
        var = self._setup(counts=counts, bias=bias)

        # Get homolog separation
        struct_bw = struct * np.repeat(var['bead_weights'], 3, axis=1)
        n = self.lengths_lowres.sum()
        hmlg_sep = ag_np.zeros(self.lengths_lowres.shape)
        begin = end = 0
        for i in range(self.lengths_lowres.size):
            end = end + ag_np.int32(self.lengths_lowres[i])
            chrom1_mean = ag_np.sum(struct_bw[begin:end], axis=0)
            chrom2_mean = ag_np.sum(struct_bw[(n + begin):(n + end)], axis=0)
            hmlg_sep_i = ag_np.sqrt(ag_np.sum(ag_np.square(
                chrom1_mean - chrom2_mean)))
            hmlg_sep = hmlg_sep.at[i].set(hmlg_sep_i)
            begin = end

        hmlg_sep_diff = self.hparams["est_hmlg_sep"] - hmlg_sep
        if 'rscale' in self.mods:
            hmlg_sep_diff = 1 - hmlg_sep / self.hparams["est_hmlg_sep"]
        if self.hparams['perc_diff'] is None:
            hmlg_sep_diff = relu(hmlg_sep_diff)
            raise ValueError("I thought we weren't doing RELU for HSC anymore")
        else:
            hsc_cutoff = ag_np.array(self.hparams['perc_diff'] * self.hparams[
                "est_hmlg_sep"])
            gt0 = hmlg_sep_diff > 0
            hmlg_sep_diff = hmlg_sep_diff.at[gt0].set(relu(
                hmlg_sep_diff[gt0] - hsc_cutoff[gt0]))
            hmlg_sep_diff = hmlg_sep_diff.at[~gt0].set(-relu(
                -(hmlg_sep_diff[~gt0] + hsc_cutoff[~gt0])))
        hmlg_sep_diff_sq = ag_np.square(hmlg_sep_diff)
        obj = ag_np.mean(hmlg_sep_diff_sq)

        if not ag_np.isfinite(obj):
            raise ValueError(f"{self.name} constraint is {obj}.")
        return self.lambda_val * obj


class BeadChainConnectivity2022(Constraint):
    """TODO"""

    def __init__(self, lambda_val, lengths, ploidy, multiscale_factor=1,
                 hparams=None, fullres_struct_nan=None, lowmem=False, mods=[]):
        self.abbrev = "bcc"
        self.name = "Bead-chain connectivity (2022)"
        self.during_alpha_infer = True
        self.lambda_val = lambda_val
        self.lengths = np.asarray(lengths)
        self.lengths_lowres = decrease_lengths_res(
            self.lengths, multiscale_factor=multiscale_factor)
        self.ploidy = ploidy
        self.multiscale_factor = multiscale_factor
        self.hparams = hparams
        self._fullres_struct_nan = fullres_struct_nan  # For self._setup() only
        self._lowmem = lowmem
        self._var = None
        self.mods = mods  # TODO remove

        self.check()

    def check(self):
        if self.ploidy == 1 and self.lambda_val > 0:
            raise ValueError(f"{self.name} constraint can not be applied to"
                             " haploid genomes.")
        if self.lambda_val < 0:
            raise ValueError("Constraint lambda may not be < 0.")
        if self.hparams is None or 'data_interchrom' not in self.hparams or (
                self.hparams['data_interchrom'] is None):
            raise ValueError(f"{self.name} constraint is missing"
                             f" neccessary hyperparams: 'data_interchrom'")

    def _setup(self, counts=None, bias=None):
        if self.lambda_val <= 0:
            return
        if self._var is not None:
            return self._var

        # Beta
        beta = _ambiguate_beta(
            [c.beta for c in counts], counts=counts, lengths=self.lengths,
            ploidy=self.ploidy)

        # Get indices of neighboring beads
        row_nghbr = _neighboring_bead_indices(
            lengths=self.lengths, ploidy=self.ploidy,
            multiscale_factor=self.multiscale_factor)

        # Get bias corresponding to neighboring beads
        row_nghbr_ambig_fullres = _neighboring_bead_indices(
            lengths=self.lengths, ploidy=1, multiscale_factor=1)
        if bias is None or np.all(bias == 1):
            bias_nghbr = 1
        else:
            bias_nghbr = bias.ravel()[row_nghbr_ambig_fullres] * bias.ravel()[
                row_nghbr_ambig_fullres + 1]

        # Get counts corresponding to neighboring beads
        if self.multiscale_factor == 1:
            raise NotImplementedError

            # TODO update this whole chunk
            counts_ambig = ambiguate_counts(
                counts=counts, lengths=self.lengths, ploidy=self.ploidy,
                exclude_zeros=False)
            counts_nghbr = counts_ambig[
                row_nghbr_ambig_fullres, row_nghbr_ambig_fullres + 1]
            counts_nghbr[np.isnan(counts_nghbr)] = np.nanmean(counts_nghbr)
            counts_nghbr = counts_nghbr.astype(_best_counts_dtype(counts_nghbr))
            counts_nghbr_mask = None
        else:
            row_nghbr_ambig_lowres = _neighboring_bead_indices(
                lengths=self.lengths, ploidy=1,
                multiscale_factor=self.multiscale_factor)
            counts_nghbr_object = sum(counts).ambiguate().filter(
                row=row_nghbr_ambig_lowres, col=row_nghbr_ambig_lowres + 1,
                copy=False)

            mask_bin_nonzero = np.isin(
                row_nghbr_ambig_lowres, counts_nghbr_object.bins_nonzero.row)
            mask_bin_zero = np.isin(
                row_nghbr_ambig_lowres, counts_nghbr_object.bins_zero.row)
            mask_no_data = (~mask_bin_nonzero) & (~mask_bin_zero)
            assert np.array_equal(
                row_nghbr_ambig_lowres[mask_bin_nonzero],
                counts_nghbr_object.bins_nonzero.row)
            assert np.array_equal(
                row_nghbr_ambig_lowres[mask_bin_zero],
                counts_nghbr_object.bins_zero.row)  # TODO remove

            # Get mask associated with neighbor beads
            counts_nghbr_mask = _get_nonzero_mask(
                multiscale_factor=self.multiscale_factor, lengths=self.lengths,
                ploidy=self.ploidy, row=row_nghbr_ambig_lowres,
                col=row_nghbr_ambig_lowres + 1,
                empty_idx_fullres=counts_nghbr_object._empty_idx_fullres)
            counts_nghbr_mask[:, mask_no_data] = _get_nonzero_mask(
                multiscale_factor=self.multiscale_factor, lengths=self.lengths,
                ploidy=self.ploidy, row=row_nghbr_ambig_lowres[mask_no_data],
                col=row_nghbr_ambig_lowres[mask_no_data] + 1,
                empty_idx_fullres=None)

            # Get counts associated with neighbor beads
            counts_nghbr = np.zeros(
                counts_nghbr_mask.shape,
                dtype=counts_nghbr_object.bins_nonzero.data.dtype)
            if not np.issubdtype(counts_nghbr.dtype, np.integer):
                print('counts_nghbr.dtype is not integer???', flush=True)  # TODO remove
            counts_nghbr[
                :, mask_bin_nonzero] = counts_nghbr_object.bins_nonzero.data

            # If an entire lowres distance bin has no counts associated with it,
            # set those counts to the mean of all high-res counts
            counts_nghbr[
                :, mask_no_data] = counts_nghbr_object.bins_nonzero.data.mean()
            counts_nghbr[~counts_nghbr_mask] = 0

        var = {
            'row_nghbr': row_nghbr, 'counts_nghbr': counts_nghbr,
            'bias_nghbr': bias_nghbr, 'beta': beta,
            'counts_nghbr_mask': counts_nghbr_mask}
        if not self._lowmem:
            self._var = var
        return var

    def apply(self, struct, alpha=None, epsilon=None, counts=None, bias=None,
              inferring_alpha=False):
        if self.lambda_val == 0 or (
                inferring_alpha and not self.during_alpha_infer):
            return 0.
        elif alpha is None:
            raise ValueError(f"Must input alpha for {self.name} constraint.")
        var = self._setup(counts=counts, bias=bias)

        if var['bias_nghbr'] is None or np.all(var['bias_nghbr'] == 1):
            bias_per_bin = 1
        else:
            bias_per_bin = np.tile(var['bias_nghbr'], 2)
        if var['counts_nghbr_mask'] is None:
            mask = None
        else:
            mask = np.tile(var['counts_nghbr_mask'], 2)
        counts_inter_mean = self.hparams['data_interchrom']['mean'] / 2

        if self.multiscale_factor == 1:
            nghbr_dis = _euclidean_distance(
                struct, row=var['row_nghbr'], col=var['row_nghbr'] + 1)
            lambda_pois = (2 * var['beta']) * bias_per_bin * ag_np.power(
                nghbr_dis, alpha)

            if 'bcc22_c_inter' in self.mods:
                lambda_pois = lambda_pois + counts_inter_mean
            obj = poisson_nll(
                np.tile(var['counts_nghbr'], 2), lambda_pois=lambda_pois)
        else:
            gamma_mean, gamma_var = get_gamma_moments(
                struct=struct, epsilon=epsilon, alpha=alpha,
                beta=var['beta'], multiscale_factor=self.multiscale_factor,
                row3d=var['row_nghbr'], col3d=var['row_nghbr'] + 1,
                stretch_fullres_beads=self.hparams['stretch_fullres_beads'],
                mean_fullres_nghbr_dis=self.hparams['mean_fullres_nghbr_dis'],
                mods=self.mods)
            gamma_mean = gamma_mean * 2
            gamma_var = gamma_var * 4

            # Adjust using inter-chromosomal counts
            if 'bcc22_c_inter' in self.mods:
                gamma_mean = gamma_mean + counts_inter_mean

            theta = gamma_var / gamma_mean
            k = ag_np.square(gamma_mean) / gamma_var
            obj = gamma_poisson_nll(
                theta=theta, k=k, data=np.tile(var['counts_nghbr'], 2),
                bias_per_bin=bias_per_bin, mask=mask, mods=self.mods)
            lambda_pois = gamma_mean  # TODO temp

        if 'debug2' in self.mods and type(obj).__name__ in ('DeviceArray', 'ndarray'):
            to_print = f"𝔼[c]={var['counts_nghbr'].mean():.2g}\t   μ={lambda_pois.mean():.2g}"
            if epsilon is not None:
                to_print += f"\t   V[c]={var['counts_nghbr'].var(axis=0).mean():.2g}"
                to_print += f"\t   σ²NB={(gamma_mean + gamma_var).mean():.2g}\t   ε={ag_np.asarray(epsilon).mean():.2g}"
            print(to_print + f"\t   OBJ={obj:.2g}", flush=True)

        if not ag_np.isfinite(obj):
            raise ValueError(f"{self.name} constraint is {obj}.")
        return self.lambda_val * obj


class HomologSeparating2022(Constraint):
    """TODO"""

    def __init__(self, lambda_val, lengths, ploidy, multiscale_factor=1,
                 hparams=None, fullres_struct_nan=None, lowmem=False, mods=[]):
        self.abbrev = "hsc"
        self.name = "Homolog separating (2022)"
        self.during_alpha_infer = True
        self.lambda_val = lambda_val
        self.lengths = np.asarray(lengths)
        self.lengths_lowres = decrease_lengths_res(
            self.lengths, multiscale_factor=multiscale_factor)
        self.ploidy = ploidy
        self.multiscale_factor = multiscale_factor
        self.hparams = hparams
        self._fullres_struct_nan = None  # Not necessary for this constraint
        self._lowmem = lowmem
        self._var = None
        self.mods = mods  # TODO remove

        self.check()

    def check(self):
        if self.ploidy == 1 and self.lambda_val > 0:
            raise ValueError(f"{self.name} constraint can not be applied to"
                             " haploid genomes.")
        if self.lambda_val < 0:
            raise ValueError("Constraint lambda may not be < 0.")
        # Hyperparam: data_interchrom
        if self.hparams is None or 'data_interchrom' not in self.hparams or (
                self.hparams['data_interchrom'] is None):
            raise ValueError(f"{self.name} constraint is missing"
                             f" neccessary hyperparams: 'data_interchrom'")

    def _setup(self, counts=None, bias=None):
        if self.lambda_val <= 0:
            return
        if self._var is not None:
            return self._var

        beta = _ambiguate_beta(
            [c.beta for c in counts], counts=counts, lengths=self.lengths,
            ploidy=self.ploidy)
        n = self.lengths_lowres.sum()
        mask_interchrom = np.invert(_intra_mask(
            (n, n), lengths_at_res=self.lengths_lowres))
        var = {'beta': beta, 'mask_interchrom': mask_interchrom}

        if not self._lowmem:
            self._var = var
        return var

    def apply(self, struct, alpha=None, epsilon=None, counts=None, bias=None,
              inferring_alpha=False):
        if self.lambda_val == 0 or (
                inferring_alpha and not self.during_alpha_infer):
            return 0.
        elif alpha is None:
            raise ValueError(f"Must input alpha for {self.name} constraint.")
        var = self._setup(counts=counts, bias=bias)

        if bias is not None and not np.all(bias == 1):
            raise NotImplementedError("bias for hsc2022")

        # Get inter-molecular indices
        n = self.lengths_lowres.sum()
        row, col = (x.ravel() for x in np.indices((n, n)))
        mask = col > row
        row_interchrom = row[mask & var['mask_interchrom']]
        col_interchrom = col[mask & var['mask_interchrom']]
        if 'hsc22_combo' in self.mods:
            row_all = ag_np.concatenate(
                [row, row_interchrom, row_interchrom + n])
            col_all = ag_np.concatenate(
                [col + n, col_interchrom, col_interchrom + n])
            idx = [[row_all, col_all]]
        else:
            row = row[mask]
            col = col[mask]
            idx_h1h2 = [row, col + n]
            idx_h2h1 = [row + n, col]
            idx_h1h1 = [row_interchrom, col_interchrom]
            idx_h2h2 = [row_interchrom + n, col_interchrom + n]
            idx = [idx_h1h2, idx_h2h1, idx_h1h1, idx_h2h2]

        # Get KL divergence
        obj = 0
        for row, col in idx:
            n, p = _get_hsc_negbinom_params(
                struct, row=row, col=col, alpha=alpha, beta=var["beta"],
                multiscale_factor=self.multiscale_factor, epsilon=epsilon, mods=self.mods)
            log_lambda_pmf = logpmf_negbinom(
                self.hparams['data_interchrom']['x'], n=n, p=p)
            obj = obj + _kl_divergence(
                p=self.hparams['data_interchrom']['y'], log_q=log_lambda_pmf)

        if 'debug' in self.mods and type(obj).__name__ in ('DeviceArray', 'ndarray'):
            row_all = ag_np.concatenate([row for row, col in idx])
            col_all = ag_np.concatenate([col for row, col in idx])
            n, p = _get_hsc_negbinom_params(
                struct, row=row_all, col=col_all, alpha=alpha, beta=var["beta"],
                multiscale_factor=self.multiscale_factor, epsilon=epsilon, mods=self.mods)
            nb_mean = (1 - p) * n / p
            nb_var = nb_mean / p

            to_print = f"KL_NB: 𝔼[c]={self.hparams['data_interchrom']['mean']:.2g}\t   NBμ={nb_mean:.2g}\t   V[c]={self.hparams['data_interchrom']['var']:.2g}\t   NBσ²={nb_var:.2g}"
            to_print += f"\t   OBJ={obj:.2g}"
            if epsilon is not None:
                to_print += f"\t   ε={ag_np.asarray(epsilon).mean():.2g}"
                if ag_np.asarray(epsilon).size > 1:
                    from .poisson import get_epsilon_per_bin
                    epsilon = get_epsilon_per_bin(
                        epsilon, row3d=row_all, col3d=col_all,
                        multiscale_factor=self.multiscale_factor)
                dis_interhmlg = _euclidean_distance(
                    struct, row=row_all, col=col_all)
                epsilon_over_dis = epsilon / dis_interhmlg
                to_print += f"\t   ε/D={epsilon_over_dis.mean():.2g}"
            print(to_print, flush=True)

        if not ag_np.isfinite(obj):
            raise ValueError(f"{self.name} constraint is {obj}.")
        return self.lambda_val * obj


def _get_hsc_negbinom_params(struct, row, col, alpha, beta, multiscale_factor=1,
                             epsilon=None, mods=[]):
    """TODO"""
    if multiscale_factor > 1:
        lambda_mean, lambda_mixture_var = get_gamma_moments(
            struct=struct, epsilon=epsilon, alpha=alpha,
            beta=beta, multiscale_factor=multiscale_factor,
            row3d=row, col3d=col, mods=mods)
        lambda_mixture_var = ag_np.mean(lambda_mixture_var)
    else:
        lambda_mean = beta * ag_np.power(
            _euclidean_distance(struct, row=row, col=col), alpha)
        lambda_mixture_var = 0

    # Get gamma params: moment matching (full-res) or mixture model (low-res)
    mean = lambda_mean.mean()
    var = lambda_mean.var() + lambda_mixture_var
    theta = var / mean
    k = ag_np.square(mean) / var

    # Mulyiply gamma distrib by 4 to account for 4 combinations across hmlgs
    theta = theta * 4

    # Compound this gamma distrib with a Poisson to yield a negbinom distrib
    n = k
    p = 1 / (theta + 1)

    return n, p


def _kl_divergence(p, log_q):
    """Measures KL divergence between two discrete distributions.

    Parameters
    ----------
    p : array
        The target probability distribution from the data.
    log_q : array
        The natural log of the approximated probability distribution.
    """
    mask = (p != 0)
    if mask.sum() == mask.size:
        tmp = p * (ag_np.log(p) - log_q)
    else:
        tmp = p[mask] * (ag_np.log(p[mask]) - log_q[mask])

    return ag_np.sum(tmp)


def prep_constraints(lengths, ploidy, multiscale_factor=1,
                     bcc_lambda=0, hsc_lambda=0, bcc_version='2019',
                     hsc_version='2019', data_interchrom=None,
                     est_hmlg_sep=None, hsc_perc_diff=None,
                     fullres_struct_nan=None, stretch_fullres_beads=None,
                     mean_fullres_nghbr_dis=None, verbose=True, mods=[]):
    """TODO"""

    # TODO remove
    if mods is None:
        mods = []
    elif isinstance(mods, str):
        mods = mods.lower().split('.')
    else:
        mods = [x.lower() for x in mods]

    if bcc_version is None:
        bcc_version = '2019'
    bcc_version = str(bcc_version)
    if hsc_version is None:
        hsc_version = '2019'
    hsc_version = str(hsc_version)

    bcc_class = {
        '2019': BeadChainConnectivity2019, '2022': BeadChainConnectivity2022}
    hsc_class = {
        '2019': HomologSeparating2019, '2022': HomologSeparating2022}
    bcc_hparams = {
        '2019': None, '2021': None,
        '2022': {'data_interchrom': data_interchrom,
                 'stretch_fullres_beads': stretch_fullres_beads,
                 'mean_fullres_nghbr_dis': mean_fullres_nghbr_dis}}
    hsc_hparams = {
        '2019': {'est_hmlg_sep': est_hmlg_sep, 'perc_diff': hsc_perc_diff},
        '2022': {'data_interchrom': data_interchrom,
                 'perc_diff': hsc_perc_diff,
                 'stretch_fullres_beads': stretch_fullres_beads,
                 'mean_fullres_nghbr_dis': mean_fullres_nghbr_dis}}

    constraints = []
    if bcc_lambda != 0:
        constraints.append(bcc_class[bcc_version](
            bcc_lambda, lengths=lengths, ploidy=ploidy,
            multiscale_factor=multiscale_factor,
            hparams=bcc_hparams[bcc_version],
            fullres_struct_nan=fullres_struct_nan, lowmem=False, mods=mods))
    if hsc_lambda != 0:
        constraints.append(hsc_class[hsc_version](
            hsc_lambda, lengths=lengths, ploidy=ploidy,
            multiscale_factor=multiscale_factor,
            hparams=hsc_hparams[hsc_version],
            fullres_struct_nan=fullres_struct_nan, lowmem=False, mods=mods))

    return constraints


def get_counts_interchrom(counts, lengths, ploidy, filter_threshold=0.04,
                          normalize=True, bias=None, verbose=True, mods=[]):
    """TODO"""

    counts, bias, lengths, _, _, _, _ = load_data(
        counts=counts, lengths_full=lengths, ploidy=ploidy,
        filter_threshold=filter_threshold, normalize=normalize, bias=bias,
        verbose=False)
    if lengths.size == 1:
        raise ValueError(
            "Must input counts_interchrom if inferring a single chromosome.")
    if bias is not None and bias.size != lengths.sum() * ploidy:
        raise ValueError("Length of bias vector does not match the counts")

    # Get inter-chromosomal ambiguated counts
    counts_ambig = ambiguate_counts(
        counts=counts, lengths=lengths, ploidy=ploidy)
    counts_interchrom = _inter_counts(
        counts_ambig, lengths_at_res=lengths, ploidy=ploidy)

    if bias is not None and not np.all(bias == 1):
        raise ValueError("What do do with this? Do I need this?")
        counts_interchrom_norm = counts_interchrom / bias.reshape(1, -1)
        counts_interchrom_norm = counts_interchrom / bias.reshape(-1, 1)

    # Get distribution of ambiguated inter-chromosomal counts
    counts_interchrom = counts_interchrom[~np.isnan(counts_interchrom)]
    counts_pmf_x = np.arange(counts_interchrom.max() + 1, dtype=int)
    bincount_y = np.bincount(counts_interchrom.astype(int))
    counts_pmf_y = bincount_y / counts_interchrom.size
    mask = counts_pmf_y != 0
    counts_pmf_x = counts_pmf_x[mask]
    counts_pmf_y = counts_pmf_y[mask]

    if verbose:
        print(f"INTER-CHROM COUNTS: mean={counts_interchrom.mean():.3g}\t"
              f"var={counts_interchrom.var():.3g}", flush=True)
    return {
        'x': counts_pmf_x, 'y': counts_pmf_y,
        'mean': counts_interchrom.mean(), 'var': counts_interchrom.var()}


def _mse_flexible(actual, expected, cutoff=None, scale_by_expected=True):
    """TODO"""

    if scale_by_expected:
        actual = actual / expected
        expected = 1

    if cutoff is not None and cutoff != 0:
        if scale_by_expected:
            window = cutoff
        else:
            window = cutoff * expected

        mle_expected = ag_np.mean(actual)

        sub_from_expected = relu_min(relu(expected - mle_expected), window)
        add_to_expected = relu_min(relu(mle_expected - expected), window)
        expected = expected + add_to_expected - sub_from_expected

    diff = expected - actual
    mse = ag_np.mean(ag_np.square(diff))

    return mse


def _mse_outside_of_window(actual, expected, cutoff=None, scale_by_expected=True):
    """TODO"""
    if scale_by_expected:
        diff = 1 - actual / expected
    else:
        diff = expected - actual

    if cutoff is None:
        diff = relu(diff)
        mse = ag_np.mean(ag_np.square(diff))
        raise ValueError("I thought we weren't doing ReLU for HSC anymore")  # TODO remove ValueError
        return mse

    if cutoff == 0:
        mse = ag_np.mean(ag_np.square(diff))
        return mse

    if scale_by_expected:
        window = cutoff
    else:
        window = cutoff * expected

    n = diff.size
    diff_gt0 = relu(diff)
    diff_lt0 = -relu(-diff)
    diff_gt0 = relu(diff_gt0 - window)
    diff_lt0 = -relu(-(diff_lt0 + window))
    mse = (ag_np.square(diff_gt0).sum() + ag_np.square(diff_lt0).sum()) / n

    return mse


def _neighboring_bead_indices(lengths, ploidy, multiscale_factor=1,
                              counts=None, include_struct_nan_beads=True):
    """Return row & col of neighboring beads, along a homolog of a chromosome.
    """

    lengths_lowres = decrease_lengths_res(lengths, multiscale_factor)
    nbeads = lengths_lowres.sum() * ploidy

    row_nghbr = np.arange(nbeads - 1, dtype=int)

    # Optionally remove beads for which there is no counts data
    if not include_struct_nan_beads:
        if counts is None:
            raise ValueError(
                "Counts must be inputted if including struct_nan beads.")
        struct_nan = find_beads_to_remove(
            counts, lengths=lengths, ploidy=ploidy,
            multiscale_factor=multiscale_factor)
        nghbr_dis_mask = (~np.isin(row_nghbr, struct_nan)) & (
            ~np.isin(row_nghbr + 1, struct_nan))
        row_nghbr = row_nghbr[nghbr_dis_mask]

    # Remove if "neighbor" beads are actually on different chromosomes
    # or homologs
    bins = np.tile(lengths_lowres, ploidy).cumsum()
    same_bin = np.digitize(row_nghbr, bins) == np.digitize(row_nghbr + 1, bins)

    row_nghbr = row_nghbr[same_bin]

    return row_nghbr


def _inter_homolog_dis(struct, lengths):
    """Computes distance between homologs for a normal diploid structure.
    """

    struct = struct.copy().reshape(-1, 3)

    n = int(struct.shape[0] / 2)
    homo1 = struct[:n, :]
    homo2 = struct[n:, :]

    hmlg_dis = []
    begin = end = 0
    for i in range(lengths.size):
        end += lengths[i]
        if np.isnan(homo1[begin:end, 0]).sum() == lengths[i] or np.isnan(
                homo2[begin:end, 0]).sum() == lengths[i]:
            hmlg_dis.append(np.nan)
        else:
            hmlg_dis.append(((np.nanmean(homo1[
                begin:end, :], axis=0) - np.nanmean(
                homo2[begin:end, :], axis=0)) ** 2).sum() ** 0.5)
        begin = end

    hmlg_dis = np.array(hmlg_dis)
    hmlg_dis[np.isnan(hmlg_dis)] = np.nanmean(hmlg_dis)

    return hmlg_dis


def _inter_homolog_dis_via_simple_diploid(struct, lengths):
    """Computes distance between chromosomes for a faux-haploid structure.
    """

    from sklearn.metrics import euclidean_distances

    struct = struct.copy().reshape(-1, 3)

    chrom_barycenters = []
    begin = end = 0
    for i in range(lengths.size):
        end += lengths[i]
        if np.isnan(struct[begin:end, 0]).sum() < lengths[i]:
            chrom_barycenters.append(
                np.nanmean(struct[begin:end, :], axis=0).reshape(1, 3))
        begin = end

    chrom_barycenters = np.concatenate(chrom_barycenters)

    hmlg_dis = euclidean_distances(chrom_barycenters)
    hmlg_dis[np.tril_indices(hmlg_dis.shape[0])] = np.nan

    return np.full(lengths.shape, np.nanmean(hmlg_dis))


def distance_between_homologs(structures, lengths, mixture_coefs=None,
                              simple_diploid=False):
    """Computes distances between homologs for a given structure.

    For diploid organisms, this computes the distance between homolog centers
    of mass for each chromosome.

    Parameters
    ----------
    structures : array of float or list of array of float
        3D chromatin structure(s) for which to assess inter-homolog distances.
    lengths : array_like of int
        Number of beads per homolog of each chromosome.
    simple_diploid: bool, optional
        For diploid organisms: whether the structure is an inferred "simple
        diploid" structure in which homologs are assumed to be identical and
        completely overlapping with one another.

    Returns
    -------
    array of float
        Distance between homologs per chromosome.

    """

    from .utils_poisson import _format_structures

    structures = _format_structures(
        structures=structures, lengths=lengths,
        ploidy=(1 if simple_diploid else 2),
        mixture_coefs=mixture_coefs)

    hmlg_dis = []
    for struct in structures:
        if simple_diploid:
            hmlg_dis.append(_inter_homolog_dis_via_simple_diploid(
                struct=struct, lengths=lengths))
        else:
            hmlg_dis.append(_inter_homolog_dis(struct=struct, lengths=lengths))

    return np.mean(hmlg_dis, axis=0)
